{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd910ec8",
   "metadata": {},
   "source": [
    "# Ejercicio práctico: Predicción de puntuación. Parte 1: Apartados de 1 a 5.\n",
    "\n",
    "Alumno: Julián María Galindo Álvarez\n",
    "\n",
    "DNI: 20081281E\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este trabajo aborda la tarea de ''Predicción de puntuación'', donde el objetivo va a ser definir una serie de modelos que puedan introducir puntuación en una entrada que carece de puntuación.\n",
    "\n",
    "En concreto, los signos de puntuación que se van a considerar son:\n",
    " * Punto: .\n",
    " * Coma: ,\n",
    " * Punto y coma: :\n",
    " * Dos puntos: :\n",
    " * Signo de exclamación: !\n",
    " * Signo de interrogación: ?\n",
    "\n",
    "A lo largo del trabajo se vana  definir dos modelos, uno de puntuación básica y otro basado en 4-gramas. Además se va a definir una serie de funciones para evaluar los modelos usando una función que implementaremos (VerifyPunctuation), basáda en la distancia de Levenshtein.\n",
    "\n",
    "Este notebook contiene los apartados del 1 al 5 del trabajo, dejando el apartado 6 en el notebook TRABAJO_PLN_APARTADO_6_JULIAN_GALINDO_ALVAREZ_20091281E y el 7 en en documento TRABAJO_PLN_APARTADO_7_JULIAN_GALINDO_ALVAREZ_20091281E.pdf\n",
    "\n",
    "Las funciones definidas en cada uno de los apartados se encuentran en los correspondientes archivos python de este directorio. En cualquier caso, este proyecto está disponible en el siguiente repositorio de github:\n",
    "\n",
    " * [github.com/julgalalv/PLN_Prediccion_de_puntuacion](https://github.com/julgalalv/PLN_Prediccion_de_puntuacion)\n",
    " \n",
    "La estructura del repositorio es la siguiente:\n",
    " * **settings.py**: parámetros globales y creación de directorios en caso necesario. \n",
    " * **preprocessor.py**: En este módulo se definen funciones que son necesarias para el procesamiento de los datos, necesarios para todos los apartados.\n",
    " * **punctuator_basic.py**: Funciones del Apartado 1 correspondientes a la puntuación básica.\n",
    " * **model_ngram.py**: Clase que define el modelo predictivo basado en N-gramas del Apartado 4.\n",
    " * **punctuator_ngram**: Funciones del Apartado 4 correspondientes a la puntuación basada en 4-gramas.\n",
    " * **evaluator.py**: Módulo que contiene las funciones para evaluar los módelos usados en el trabajo, esto es, los métodos correspondientes a los Apartados 2, 3 y 5.\n",
    "\n",
    "En cuanto a los directorios tenemos:\n",
    "\n",
    "    -root\n",
    "        |-data  \n",
    "        |  |-prepared\n",
    "        |  |-preprocessed\n",
    "        |  |-raw\n",
    "        |-predicted\n",
    "        |-punctuator2tf2\n",
    "        |-zips\n",
    "\n",
    " * **data**: Contiene subdirectorios tando de datos del dataser original como procesados.\n",
    " * **raw**: Contiene los corpus de dataset originales.\n",
    " * **prepared**: Contiene archivos generados de preprocesamiento de los corpus (apartado 6)\n",
    " * **preprocessed**: Contiene archivos generados de preprocesamiento de los corpus vectorizados para el modelo de Ottokar Tilk y Tanel Alum (apartado 6).\n",
    " * **predicted**: Contiene archivos con predicciones (puntuaciones) realizadas por los modelos.\n",
    " * **punctuator2tf2**: Modelo de Ottokar Tilk y Tanel Alum (apartado 6), implementación en TensorFLow con modificaciones\n",
    " realizadas por mí para adaptalo a este trabajo.\n",
    " * **zips**: contiene el dataset y el modelo punctuator2tf2 comprimidos.\n",
    " \n",
    "**NOTA**: A lo largo del trabajo voy a ir importando las funciones de cada archivo conforma hagan falta, recomiendo ir al código para consultarlas conforme se avanza en este documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75e6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66fc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos parámetros globales y creamos directorios en caso necesario\n",
    "settings.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028bb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos variables con las rutas de los archivos de corpus\n",
    "TEST_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.test.en')\n",
    "CHECK_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.check.en')\n",
    "TRAIN_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.train.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec49d2c",
   "metadata": {},
   "source": [
    "## APARTADO 1: Modelo de puntuación básica. addPunctuationBasic.\n",
    "\n",
    "Vamos a implementar un sistema que reciba una expresión como entrada (será una expresión formada solo por minúsculas y sin los signos de puntuación mencionados) y la salida será la misma expresión pero con los cambios correspondientes a la introducción de mayúsculas y signos de puntuación indicados.\n",
    "\n",
    "Como primera versión de esta función addPunctuationBasic se implementará un modelo que\n",
    "simplemente cambia la primera letra por mayúscula y añade al final del string de entrada un punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a142e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punctuator_basic import addPunctuationBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b8a98",
   "metadata": {},
   "source": [
    "Vemos que la función anterior realiza la operación correctamente con el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25868085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esta es una frase de prueba.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addPunctuationBasic('esta es una frase de prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb097359",
   "metadata": {},
   "source": [
    "## APARTADO 2: Verificación. VerifyPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da97fae",
   "metadata": {},
   "source": [
    "Antes de definir la función verifyPuntuation vamos a definir dos funciones auxiliares que nos servirán.\n",
    "* padding(list1,list2): Devuelve las los listas de entrada de forma que ambas tengan la misma longitud, añadiendo el elemento string vacío ('') a la lista de menor longitud.\n",
    "\n",
    "* tokenizer(text): tokeniza el texto de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754f48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import padding, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8162c1",
   "metadata": {},
   "source": [
    "Veamos el funcionamiento de *padding* con el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2ce2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista 1 con padding:  ['s', 'a', '', ''] \t Lista 2 con padding:  ['1', '2', '3', '4']\n",
      "Las longitudes son iguales:  True\n"
     ]
    }
   ],
   "source": [
    "list1, list2 = ['s','a'], ['1','2','3','4']\n",
    "l1, l2 = padding(list1,list2)\n",
    "print('Lista 1 con padding: ',l1,'\\t Lista 2 con padding: ', l2)\n",
    "print('Las longitudes son iguales: ', len(l1) == len(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf854a0",
   "metadata": {},
   "source": [
    "Veamos cómo funciona *tokenizer* con el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sara', 'said', ':', 'Hello', ',', \"what's\", 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "text_example= \"Sara said: Hello, what's your name?\"\n",
    "print(tokenizer(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f20f0d",
   "metadata": {},
   "source": [
    "Ahora vamos a importar la función principal de este apartado, *verifyPunctuation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80814bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import verifyPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fbedc",
   "metadata": {},
   "source": [
    "Utilicemos el ejemplo del documento para ver si funciona correctamente la función. Además verificamos que intercambiar check y test devuelve el mismo número de elementos (distancia de Levenshtein)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f21d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check vs test:  {('D', 1), ('S', 2), ('I', 4)}\n",
      "test vs check:  {('S', 1), ('I', 1), ('D', 3)}\n"
     ]
    }
   ],
   "source": [
    "check_example = \"Hello. What's your name?\"\n",
    "test_example = \"Hello what's your, name?\"\n",
    "print('check vs test: ', verifyPunctuation(check_example,test_example))\n",
    "print('test vs check: ', verifyPunctuation(test_example,check_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3077",
   "metadata": {},
   "source": [
    "## APARTADO 3: Evaluación de modelos.\n",
    "\n",
    "Implementaremos una herramienta que permita recorrer todo el corpus de test y verificación. Es decir, irá recorriendo una a una las líneas de cada fichero (que están alineadas), aplicaría sobre la frase de test el algoritmo básico de puntuación (apartado 1: *addPunctuationBasic* ) y a continuación comprobaría si el resultado es o no correcto usando la función *verifyPunctuation* del apartado 2.\n",
    "\n",
    "En primer lugar vamos a definir una función *evaluate_example* que calcula las métricas precision y recall dado una instancia (check,test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6771be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bf505",
   "metadata": {},
   "source": [
    "Comprobamos el funcionamiento de la función con el siguiente ejemplo.\n",
    "\n",
    "* check = \"Hello. What's your name?\"\n",
    "* test = \"hello what's your name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7de147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LINE: \n",
      "  hello what's your name\n",
      "MODEL PUNCTUATED LINE: \n",
      "  Hello what's your name.\n",
      "VALIDATION LINE: \n",
      "  Hello. What's your name? \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('S', 1), ('I', 1), ('I', 4)}\n",
      "Modificaciones hechas por el modelo:  {('S', 0), ('I', 4)}\n",
      "Diferencias entre modelo y validación:  {('S', 1), ('I', 1), ('S', 4)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  2\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  4 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.25 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.5, 0.25, 2, 1, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_example = \"Hello. What's your name?\"\n",
    "test_example = \"hello what's your name\"\n",
    "evaluate_example(addPunctuationBasic,check_example,test_example,print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c460dae",
   "metadata": {},
   "source": [
    "Notemos que la oración puntuada es \"Hello what's your name.\", por lo que la función de validación va a considerar como correcto el haber insertado en el token 4, lo que se observa como ('I', 4) tanto en las modificaciones necesarias como las hechas por el modelo. Al no tener información sobre el caracter, esto podría dar lugar a un falso positivo, que es corregido correctamente con el error de sustitución de signo (ver función).\n",
    "\n",
    "Definamos ahora la función principal de este apartado, *evaluate*, que raliza el proceso anterior para distintos corpus y calcula métricas gobales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42740c1b",
   "metadata": {},
   "source": [
    "Evaluamos la función *addPunctuationBasic*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e40008f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.9482794650320423\n",
      "recall global:  0.42824561955393375\n",
      "F1 global:  0.5900314226893488\n",
      "=============================================\n",
      "precision media:  0.9475733555833681\n",
      "recall medio:  0.587400667208851\n",
      "F1 medio:  0.7252308026509773\n",
      "=============================================\n",
      "rendimiento:  0.263384786538729\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_global': 0.9482794650320423,\n",
       " 'recall_global': 0.42824561955393375,\n",
       " 'F1_global': 0.5900314226893488,\n",
       " 'precision_mean': 0.9475733555833681,\n",
       " 'recall_mean': 0.587400667208851,\n",
       " 'F1_mean': 0.7252308026509773,\n",
       " 'score': 0.263384786538729}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(addPunctuationBasic, check_file_path=CHECK_RAW_PATH, test_file_path=TEST_RAW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cd307",
   "metadata": {},
   "source": [
    "Vemos que la precisión es alta, lo que indica que lo que tiene que hacer el modelo (poner la primera letra en mayúscula y un punto al final), lo hace bien. Sin embargo, el recall es relativamente bajo ya que esos cambios no son suficientes para puntuar correctamente las oraciones, cosa que también se deduce del bajo rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6c1ac",
   "metadata": {},
   "source": [
    "Aunque no se pide, definimos la función evaluate_example_from_corpus que permite ver la información resultante de evaluate_example para un elemento en concreto del corpus dado el número de línea corpus_line. De esta forma podemos explorar y verificar el correcto funcionamiento de las funciones definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edded325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate_example_from_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b6cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LINE: \n",
      "  people working in these canneries could barely stay there all day because of the smell but you know what they came out saying\n",
      "MODEL PUNCTUATED LINE: \n",
      "  People working in these canneries could barely stay there all day because of the smell but you know what they came out saying.\n",
      "VALIDATION LINE: \n",
      "  People working in these canneries could barely stay there all day because of the smell, but you know what they came out saying? \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('I', 23), ('I', 15)}\n",
      "Modificaciones hechas por el modelo:  {('S', 0), ('I', 23)}\n",
      "Diferencias entre modelo y validación:  {('S', 23), ('I', 15)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  2\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.3333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_line = 2\n",
    "evaluate_example_from_corpus(addPunctuationBasic,check_file_path=CHECK_RAW_PATH,test_file_path=TEST_RAW_PATH,\\\n",
    "                             corpus_line=corpus_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614e4d2",
   "metadata": {},
   "source": [
    "## APARTADO 4. Modelo basado en 4-gramas. addPunctuation4gram.\n",
    "\n",
    "Utilizando el corpus de entrenamiento contenido en PunctuationTask.train.en construimos un modelo de lenguaje inspirado en la idea de 4-gramas.\n",
    "\n",
    "Definimos una función auxiliar *ngrams* que dado el string text devuelve la lista de N-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f67d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8720",
   "metadata": {},
   "source": [
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca0d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said'), ('said', ':'), (':', 'Hello'), ('Hello', '!'), ('!', 'nice'), ('nice', 'to'), ('to', 'meet'), ('meet', 'you'), ('you', '!')]\n",
      "3-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said', ':'), ('said', ':', 'Hello'), (':', 'Hello', '!'), ('Hello', '!', 'nice'), ('!', 'nice', 'to'), ('nice', 'to', 'meet'), ('to', 'meet', 'you'), ('meet', 'you', '!')]\n",
      "4-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said', ':', 'Hello'), ('said', ':', 'Hello', '!'), (':', 'Hello', '!', 'nice'), ('Hello', '!', 'nice', 'to'), ('!', 'nice', 'to', 'meet'), ('nice', 'to', 'meet', 'you'), ('to', 'meet', 'you', '!')]\n"
     ]
    }
   ],
   "source": [
    "text_example = 'Sara said: Hello! nice to meet you!'\n",
    "\n",
    "# Veamos los Ngramas con N de 2 a 4\n",
    "for n in range(2,5):\n",
    "    print(str(n)+'-gramas de la oración: ', text_example)\n",
    "    print('\\t', ngrams(text_example,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f18e57",
   "metadata": {},
   "source": [
    "Creamos la clase **ModelNgram**, que puede generalizarse fácilmente y que instanciaremos con N=4 para definir el modelo propuesto del apartado 4.\n",
    "\n",
    "Esta clase tiene un método *entrena* para entrenar el modelo a partir de un archivo, un método *predice* que devuelve la operación más probable dada un N-1 tupla. Ademas como atributos cuenta con dos diccionarios:\n",
    "\n",
    " * **counts_dict**: para cada operación, cuenta con un diccionario donde cada clave es un N-1 tupla del corpus y su valor es el número de veces que aparece esa tupla precedida de la operación.\n",
    " * **trained_dict**: cada clave es una N-1 tupla del corpus de entrenamiento y su valor es la operación más probable (la que cuenta con mayorres repeticiones en count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fb5eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ngram import ModelNgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b80dd0",
   "metadata": {},
   "source": [
    "Instanciamos el modelo y lo entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da6e256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4gram = ModelNgram(N=4)\n",
    "model4gram.entrena(train_file_path=TRAIN_RAW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb0657",
   "metadata": {},
   "source": [
    "Consultamos el diccionario de conteo para comprobar la distribución de signos dada la terna ('by','the','way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f57fbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocurrencias con el signo  <mayus>  para la terna  ('by', 'the', 'way') : 0\n",
      "Ocurrencias con el signo  <minus>  para la terna  ('by', 'the', 'way') : 25\n",
      "Ocurrencias con el signo  .  para la terna  ('by', 'the', 'way') : 36\n",
      "Ocurrencias con el signo  ,  para la terna  ('by', 'the', 'way') : 200\n",
      "Ocurrencias con el signo  ;  para la terna  ('by', 'the', 'way') : 1\n",
      "Ocurrencias con el signo  :  para la terna  ('by', 'the', 'way') : 0\n",
      "Ocurrencias con el signo  ?  para la terna  ('by', 'the', 'way') : 1\n",
      "Ocurrencias con el signo  !  para la terna  ('by', 'the', 'way') : 0\n"
     ]
    }
   ],
   "source": [
    "terna = ('by','the','way')\n",
    "for i in model4gram.counts_dict:\n",
    "    print('Ocurrencias con el signo ',i,' para la terna ',terna, ':',model4gram.counts_dict[i].get(terna,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c21126",
   "metadata": {},
   "source": [
    "Se observa que la mayor ocurrencia se da para la coma ',' por lo que la función *predice* del modelo debe devolver dicho caracter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cde1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación más probable dada la terna  ('by', 'the', 'way')  es:  ,\n"
     ]
    }
   ],
   "source": [
    "print('La operación más probable dada la terna ',terna, ' es: ',model4gram.predice(terna))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d38999",
   "metadata": {},
   "source": [
    "Igual que definimos el modelo de forma genérica para Ngramas, vamos a definir la función genérica addPunctuationNgram donde la función requerida por el apartado addPunctuation4gram es addPunctuationNgram usando un modelo basado en 4 gramas.\n",
    "\n",
    "Esta función cuenta con un parámetro *add_punc_basic* que añade puntuación básica como la de addPuntuationBasic ya que el modelo de 4 gramas nunca va a poner la primera letra en mayúsculas y puede que deje la oración sin puntuación final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53f7eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punctuator_ngram import addPunctuationNgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91219f40",
   "metadata": {},
   "source": [
    "Podemos definir ahora la función de puntuación addPunctuation4gram simplemente como la ejecución de addPunctuationNgram\n",
    "verificando que el modelo usado usa 4gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2490e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPunctuation4gram(model,example,add_basic_punct = False):\n",
    "    N = model.N\n",
    "    assert N == 4, 'The model is based in {} -grams and it should be 4-grams.'.format(str(N))\n",
    "    return addPunctuationNgram(model,example,add_basic_punct = add_basic_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1944aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase sin puntuar:\n",
      " \t and we also are eating meat that comes from some of these same places\n",
      "Puntuación de la frase con modelo 4 gramas:\n",
      " \t and we also are eating meat, that comes from some of these same places\n",
      "Puntuación de la frase con modelo 4 gramas + addPunctuationBasic:\n",
      " \t And we also are eating meat, that comes from some of these same places.\n"
     ]
    }
   ],
   "source": [
    "text_example = \"and we also are eating meat that comes from some of these same places\"\n",
    "print('Frase sin puntuar:\\n \\t',text_example)\n",
    "print('Puntuación de la frase con modelo 4 gramas:\\n \\t',addPunctuation4gram(model4gram,text_example,add_basic_punct=False))\n",
    "print('Puntuación de la frase con modelo 4 gramas + addPunctuationBasic:\\n \\t',addPunctuation4gram(model4gram,text_example,add_basic_punct=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8975744",
   "metadata": {},
   "source": [
    "Exploremos algunos ejemplos usando puntuación básica y sin ella en el modelo de 4gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957ba0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODELO 4GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  it can be a very complicated thing, the ocean\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Modificaciones hechas por el modelo:  {('I', 7)}\n",
      "Diferencias entre modelo y validación:  {('S', 0), ('I', 10)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  1\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  0.3333333333333333 \n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "4GRAMS + PUNTUACION BÁSICA\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing, the ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Modificaciones hechas por el modelo:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Diferencias entre modelo y validación:  set() \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  3\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  3\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  1.0 \n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instancia del corpus. Modificar para ver distintos ejemplos.\n",
    "i = 0\n",
    "\n",
    "l = 70\n",
    "print('='*l)\n",
    "print('MODELO 4GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuation4gram, model = model4gram,test_file_path=TEST_RAW_PATH,\\\n",
    "                             check_file_path= CHECK_RAW_PATH, add_punct_basic=False,corpus_line = i)\n",
    "print('='*l)\n",
    "print('='*l)\n",
    "print('4GRAMS + PUNTUACION BÁSICA')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuation4gram, model = model4gram, test_file_path=TEST_RAW_PATH,\\\n",
    "                             check_file_path= CHECK_RAW_PATH,add_punct_basic=True,corpus_line = i)\n",
    "print('='*l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56a6cb",
   "metadata": {},
   "source": [
    "## APARTADO 5: Evaluación de addPunctuation4gram y comparación de modelos.\n",
    "\n",
    "Evaluemos el modelo usando la misma función *evaluate* anterior. Vamos a comparar el rendimiento usando también la puntuación básica y sin ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c01512bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELO 4GRAMS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.3061919504643963\n",
      "recall global:  0.04666708609896505\n",
      "F1 global:  0.08099033684555332\n",
      "=============================================\n",
      "precision media:  0.14064391487397296\n",
      "recall medio:  0.04956605113889393\n",
      "F1 medio:  0.07329966587077798\n",
      "=============================================\n",
      "rendimiento:  0.0\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MODELO 4GRAMS')\n",
    "evaluate(addPunctuation4gram,model=model4gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c333949",
   "metadata": {},
   "source": [
    "Podemos ver que este modelo tal cual es muy pobre. Usando la función *evaluate_example_from_corpus*, podemos ver que nunca pone la mayúscula inicial como era de esperar y rara vez un punto al final. Además las predicciones parecen bastante pobres y esto se puede deber a la falta de variedad en en las tuplas del corpus de entrenamiento. Una posible mejoría se conseguiría con un corpus más grande y variado.\n",
    "\n",
    "Veamos la evaluación añadiendo la puntuación básica y lo comparamos con lo obtenido en addPunctuationBasic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4876526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4GRAMS + PUNTUACION BÁSICA\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.7801183367117734\n",
      "recall global:  0.447937965963069\n",
      "F1 global:  0.569101954358339\n",
      "=============================================\n",
      "precision media:  0.8433161902915692\n",
      "recall medio:  0.5991575233871246\n",
      "F1 medio:  0.7005732377871406\n",
      "=============================================\n",
      "rendimiento:  0.2369628702544848\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n",
      "PUNTUACIÓN BÁSICA\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.9482794650320423\n",
      "recall global:  0.42824561955393375\n",
      "F1 global:  0.5900314226893488\n",
      "=============================================\n",
      "precision media:  0.9475733555833681\n",
      "recall medio:  0.587400667208851\n",
      "F1 medio:  0.7252308026509773\n",
      "=============================================\n",
      "rendimiento:  0.263384786538729\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('4GRAMS + PUNTUACION BÁSICA')\n",
    "evaluate(addPunctuation4gram,model=model4gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,\\\n",
    "         add_punct_basic=True)\n",
    "print()\n",
    "print('PUNTUACIÓN BÁSICA')\n",
    "evaluate(addPunctuationBasic,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ee6ed",
   "metadata": {},
   "source": [
    "Podemos ver que en general la precisión es más baja en el modelo de 4gramas pero el recall ligeramente superior, pero no de forma sustancial. Además los F1 son menores en el modelo de puntuación básica que en el de 4 gramas así como el rendimiento.\n",
    "\n",
    "En general podemos concluir que el modelo de puntuación básica es ligeramente mejor que el basado en 4gramas cuando se añade la puntuación línea a línea por sorprendente que parezca. Esto puede deverse a que muchas de las oraciones del corpus de entrenamiento no presentan signos intermedios (sólo mayúscula inicial y punto final, exactamente lo que hace addPunctuationBasic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02447dfb",
   "metadata": {},
   "source": [
    "Ya que hemos implementado un modelo genérico, vamos a comparar con modelos basados en 3gramas y 5gramas para ver si existe mejoría o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df5a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3gram = ModelNgram(N=3)\n",
    "model5gram = ModelNgram(N=5)\n",
    "model3gram.entrena(train_file_path=TRAIN_RAW_PATH)\n",
    "model5gram.entrena(train_file_path=TRAIN_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c0b5296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3GRAMAS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.7269191453289785\n",
      "recall global:  0.45858630343829626\n",
      "F1 global:  0.5623848698486792\n",
      "=============================================\n",
      "precision media:  0.8041213477542051\n",
      "recall medio:  0.6055814495149251\n",
      "F1 medio:  0.6908704051694091\n",
      "=============================================\n",
      "rendimiento:  0.2259769155889306\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_global': 0.7269191453289785,\n",
       " 'recall_global': 0.45858630343829626,\n",
       " 'F1_global': 0.5623848698486792,\n",
       " 'precision_mean': 0.8041213477542051,\n",
       " 'recall_mean': 0.6055814495149251,\n",
       " 'F1_mean': 0.6908704051694091,\n",
       " 'score': 0.2259769155889306}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3GRAMAS')\n",
    "evaluate(addPunctuationNgram, model = model3gram,test_file_path=TEST_RAW_PATH,\\\n",
    "         check_file_path= CHECK_RAW_PATH, add_punct_basic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b82155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5GRAMAS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.8715371568565865\n",
      "recall global:  0.43644027808361385\n",
      "F1 global:  0.581621530980129\n",
      "=============================================\n",
      "precision media:  0.9022099935324751\n",
      "recall medio:  0.592608753166226\n",
      "F1 medio:  0.7153476507331434\n",
      "=============================================\n",
      "rendimiento:  0.2533027395355305\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('5GRAMAS')\n",
    "evaluate(addPunctuationNgram, model = model5gram, test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,add_punct_basic=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06a197",
   "metadata": {},
   "source": [
    "Observamos que en cuanto a precision, el modelo basado en 3gramas es ligeramente inferior al de 4gramas y este a su vez inferior al de 5 gramas, todos por debajo de la precisión del puntuador básico. En cuanto a recall el orden es inverso: el modelo basado en 3 gramas presenta mayor recall que el de 4 y este a su vez que el de 5.\n",
    "En cuanto al F1, los mayores valores los encontramos para el modelo de 5gramas y de puntuación básica. Es obvio que cuanto mayor sea N, menos probable es la probabilidad de que se de una N-tupla concreta por lo que el modelo basado en N-gramas con N grande coincidirá eventualmente con el de puntuación básica si se considera el parámetro add_punct_basic.\n",
    "\n",
    "Para acabar este apartado podemos explorar algunas predicciones de los modelos de 3 y 5 gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a10a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODELO 3GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing. The ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Modificaciones hechas por el modelo:  {('S', 0), ('S', 7), ('I', 7), ('I', 9)}\n",
      "Diferencias entre modelo y validación:  {('S', 7), ('S', 8)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  4\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  2\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.6666666666666666 \n",
      "\n",
      "======================================================================\n",
      "MODELO 5GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing, the ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Modificaciones hechas por el modelo:  {('S', 0), ('I', 7), ('I', 9)}\n",
      "Diferencias entre modelo y validación:  set() \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  3\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  3\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instancia del corpus. Modificar para ver distintos ejemplos.\n",
    "i= 0\n",
    "\n",
    "l = 70 \n",
    "print('='*l)\n",
    "print('MODELO 3GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuationNgram, model = model3gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH, add_punct_basic=True,corpus_line = i)\n",
    "print('='*l)\n",
    "print('MODELO 5GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuationNgram, model = model5gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH, add_punct_basic=True,corpus_line = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a6055",
   "metadata": {},
   "source": [
    "Fin de este documento. El apartado 6 se puede seguir en el siguiente Notebook. Esta separación se debe a dos cosas:\n",
    " * La distinta naturaleza de los apartados (implementación y desarrollo frente a exploración y adaptación)\n",
    " * El notebook del apartado 6 está pensado para ser ejecutado a parte en Google Collab con GPU ya que el modelo tarda mucho en entrenar y puntuar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf8521805df2f681626d7ffc655f9a444f7479c982c19b2313015333928fe69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
