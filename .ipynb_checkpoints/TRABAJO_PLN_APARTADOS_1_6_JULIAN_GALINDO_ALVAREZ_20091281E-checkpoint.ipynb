{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd910ec8",
   "metadata": {},
   "source": [
    "# Ejercicio práctico: Predicción de puntuación. \n",
    "\n",
    "**Alumno**: Julián María Galindo Álvarez\n",
    "\n",
    "**DNI**: 20081281E\n",
    "\n",
    "# Parte 1: Apartados de 1 a 5.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este trabajo aborda la tarea de ''Predicción de puntuación'', donde el objetivo va a ser definir una serie de modelos que puedan introducir puntuación en una entrada que carece de puntuación.\n",
    "\n",
    "En concreto, los signos de puntuación que se van a considerar son:\n",
    " * Punto: .\n",
    " * Coma: ,\n",
    " * Punto y coma: :\n",
    " * Dos puntos: :\n",
    " * Signo de exclamación: !\n",
    " * Signo de interrogación: ?\n",
    "\n",
    "A lo largo del trabajo se vana  definir dos modelos, uno de puntuación básica y otro basado en 4-gramas. Además se va a definir una serie de funciones para evaluar los modelos usando una función que implementaremos (VerifyPunctuation), basáda en la distancia de Levenshtein.\n",
    "\n",
    "Este notebook contiene los apartados del 1 al 6 del trabajo, dejando el apartado 7 en en documento TRABAJO_PLN_APARTADO_7_JULIAN_GALINDO_ALVAREZ_20091281E.pdf\n",
    "\n",
    "Las funciones definidas en cada uno de los apartados se encuentran en los correspondientes archivos python de este directorio. En cualquier caso, este proyecto está disponible en el siguiente repositorio de github:\n",
    "\n",
    " * [github.com/julgalalv/PLN_Prediccion_de_puntuacion](https://github.com/julgalalv/PLN_Prediccion_de_puntuacion)\n",
    " \n",
    "La estructura del repositorio es la siguiente:\n",
    " * **settings.py**: parámetros globales y creación de directorios en caso necesario. \n",
    " * **preprocessor.py**: En este módulo se definen funciones que son necesarias para el procesamiento de los datos, necesarios para todos los apartados.\n",
    " * **punctuator_basic.py**: Funciones del Apartado 1 correspondientes a la puntuación básica.\n",
    " * **model_ngram.py**: Clase que define el modelo predictivo basado en N-gramas del Apartado 4.\n",
    " * **punctuator_ngram**: Funciones del Apartado 4 correspondientes a la puntuación basada en 4-gramas.\n",
    " * **evaluator.py**: Módulo que contiene las funciones para evaluar los módelos usados en el trabajo, esto es, los métodos correspondientes a los Apartados 2, 3 y 5.\n",
    "\n",
    "En cuanto a los directorios tenemos:\n",
    "\n",
    "    -root\n",
    "        |-data  \n",
    "        |  |-prepared\n",
    "        |  |-preprocessed\n",
    "        |  |-raw\n",
    "        |-predicted\n",
    "        |-punctuator2tf2\n",
    "        |-zips\n",
    "\n",
    " * **data**: Contiene subdirectorios tando de datos del dataser original como procesados.\n",
    " * **raw**: Contiene los corpus de dataset originales.\n",
    " * **prepared**: Contiene archivos generados de preprocesamiento de los corpus (apartado 6)\n",
    " * **preprocessed**: Contiene archivos generados de preprocesamiento de los corpus vectorizados para el modelo de Ottokar Tilk y Tanel Alum (apartado 6).\n",
    " * **predicted**: Contiene archivos con predicciones (puntuaciones) realizadas por los modelos.\n",
    " * **punctuator2tf2**: Modelo de Ottokar Tilk y Tanel Alum (apartado 6), implementación en TensorFLow con modificaciones\n",
    " realizadas por mí para adaptalo a este trabajo.\n",
    " * **zips**: contiene el dataset y el modelo punctuator2tf2 comprimidos.\n",
    " \n",
    "**NOTA**: A lo largo del trabajo voy a ir importando las funciones de cada archivo conforma hagan falta, comentando el código en el propio notebook. En cualquie caso recomiendo ir al código (que está completamente comentado) conforme se avanza en este documento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2c085",
   "metadata": {},
   "source": [
    "En primer lugar se ha definido un módulo **settings** para definir parámetros globales y crear directorios en caso necesario a través de la función *initialize*.\n",
    "\n",
    "```\n",
    "def initialize():\n",
    "    global PUNCT_MARK_DICT      # diccionario con el mapeo de los signos de puntuación y tokens. Usado en el apartado 6\n",
    "    global PUNCT_MARKS          # lista con signos de puntuación ['.',',',';',':','?','!']\n",
    "    global MAYUS_MARKS          # lista de signos de puntuación de final de oración ['.','?','!']\n",
    "    ...\n",
    "    global CURRENT_DIR              # ruta a directorio raíz\n",
    "    global DATA_DIR                 # ruta a directorio de datos\n",
    "    global DATA_RAW_DIR             # ruta a directorio de archivos del dataset \n",
    "    global DATA_PREPROCESSED_DIR    # ruta a directorio de archivos de datos preprocesados (con signos tokenizados).\n",
    "    global DATA_PREPARED_DIR        # ruta a directorio de archivos procesados (Apartado 6)\n",
    "\n",
    "    global PUNCTUATOR2TF2_DIR      # ruta al directorio de archivos del modelo de Ottokar Tilk y Tanel Alum (Apartado 6)\n",
    "    global PREDICTED_DIR            # ruta al directorio de archivos con predicciones de modelos\n",
    "    ...\n",
    "\n",
    "    PUNCT_MARK_DICT = {\".\": \".PERIOD\", \",\": \",COMMA\", \";\": \";SEMICOLON\", \":\": \":COLON\", \"?\": \"?QUESTIONMARK\", \n",
    "                       \"!\":\"!EXCLAMATIONMARK\"}\n",
    "    PUNCT_MARKS = ['.',',',';',':','?','!'] # Signos de puntuación (sdp)\n",
    "    MAYUS_MARKS = ['.','?','!'] # Signos de final de oración\n",
    "    ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75e6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66fc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos parámetros globales y creamos directorios en caso necesario\n",
    "settings.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028bb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos variables con las rutas de los archivos de corpus\n",
    "TEST_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.test.en')\n",
    "CHECK_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.check.en')\n",
    "TRAIN_RAW_PATH = os.path.join(settings.DATA_RAW_DIR,'PunctuationTask.train.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec49d2c",
   "metadata": {},
   "source": [
    "## APARTADO 1: Modelo de puntuación básica. addPunctuationBasic.\n",
    "\n",
    "Vamos a implementar un sistema que reciba una expresión como entrada (será una expresión formada solo por minúsculas y sin los signos de puntuación mencionados) y la salida será la misma expresión pero con los cambios correspondientes a la introducción de mayúsculas y signos de puntuación indicados.\n",
    "\n",
    "Como primera versión de esta función addPunctuationBasic se implementará un modelo que\n",
    "simplemente cambia la primera letra por mayúscula y añade al final del string de entrada un punto.\n",
    "\n",
    "```\n",
    "def addPunctuationBasic(string):\n",
    "    \"\"\"\n",
    "    Función principal del Apartado 1. Pone en mayúscula la primera letra \n",
    "    del string  y añade un punto '.' al final si no lo tiene.\n",
    "\n",
    "    Devuelve el string puntuado.\n",
    "    \"\"\"\n",
    "    initial_upper = change_initial(string,uppercase=True)\n",
    "    last_char = initial_upper[-1]\n",
    "    add_dot = '.' if last_char not in settings.PUNCT_MARKS else ''\n",
    "    return initial_upper + add_dot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a142e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punctuator_basic import addPunctuationBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b8a98",
   "metadata": {},
   "source": [
    "Vemos que la función anterior realiza la operación correctamente con el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25868085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esta es una frase de prueba.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addPunctuationBasic('esta es una frase de prueba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb097359",
   "metadata": {},
   "source": [
    "## APARTADO 2: Verificación. VerifyPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da97fae",
   "metadata": {},
   "source": [
    "Antes de definir la función verifyPuntuation vamos a definir dos funciones auxiliares que nos servirán.\n",
    "* **padding(list1,list2)**: Devuelve las los listas de entrada de forma que ambas tengan la misma longitud, añadiendo el elemento string vacío ('') a la lista de menor longitud.\n",
    "\n",
    "* **tokenizer(text)**: tokeniza el texto de entrada.\n",
    "\n",
    "```\n",
    "def tokenizer(text,prepared = False):\n",
    "    \"\"\"\n",
    "    Función que tokeniza un string\n",
    "\n",
    "    input: \n",
    "        text: string a procesar\n",
    "        prepared: si False, usa la expresión regular  r'?([.|,|;|:|!|?]+) ?' para añadir un espacio\n",
    "                  a los signos de puntuación para poder ser separados mediante str.split(). En caso contrario el\n",
    "                  texto ya viene preprocesado (APARTADO 6) y solo es necesario hacer el split(). \n",
    "\n",
    "    output: \n",
    "        lista de tokens de un string\n",
    "    \"\"\"\n",
    "\n",
    "    if not prepared:\n",
    "        # Generamos la expresión regular que añade espacios a los signos para poder separarlos con el método split()\n",
    "        marks = ''.join(settings.PUNCT_MARKS)\n",
    "        regex1, regex2 = r' ?([{}]+) ?'.format(marks), r' \\1 '\n",
    "        text = re.sub(regex1, regex2, text)\n",
    "    return text.split()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754f48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import padding, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8162c1",
   "metadata": {},
   "source": [
    "Veamos el funcionamiento de *padding* con el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2ce2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista 1 con padding:  ['s', 'a', '', ''] \t Lista 2 con padding:  ['1', '2', '3', '4']\n",
      "Las longitudes son iguales:  True\n"
     ]
    }
   ],
   "source": [
    "list1, list2 = ['s','a'], ['1','2','3','4']\n",
    "l1, l2 = padding(list1,list2)\n",
    "print('Lista 1 con padding: ',l1,'\\t Lista 2 con padding: ', l2)\n",
    "print('Las longitudes son iguales: ', len(l1) == len(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf854a0",
   "metadata": {},
   "source": [
    "Veamos cómo funciona *tokenizer* con el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d12cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sara', 'said', ':', 'Hello', ',', \"what's\", 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "text_example= \"Sara said: Hello, what's your name?\"\n",
    "print(tokenizer(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f20f0d",
   "metadata": {},
   "source": [
    "Ahora vamos a importar la función principal de este apartado, *verifyPunctuation*. Para definir esta función tokenizamos los strings test y check y hacemos padding para que ambas listas tengan la misma longitud. La idea tras esta función es procesar token a token de izquierda a derecha, comprobando si se ha realizado una eliminación o inserción de manera que se realiza la operación correspondiente en test hasta que ambas listas contienen el mismo número de tokens (no vacíos del padding). Tras esto, se comparan token a token para detectar las sustituciones\n",
    "\n",
    "```\n",
    "def verifyPunctuation(check, test, prepared = False):\n",
    "    \"\"\"\n",
    "    Función principal de Apartado 2. Dados un string check y test, devuelve las operaciones\n",
    "    necesarias para llegar de test a check (con índice de token referente a check). \n",
    "    Las operaciones son:\n",
    "        * (D,i) (Delete): en test se ha eliminado el token i de check.\n",
    "        * (I,i) (Insertion): en test se ha insertado un token en la posción i de check.\n",
    "        * (S,i) (Substitution): en test existe un token distinto con respecto al token i de check.\n",
    "\n",
    "    \"\"\"\n",
    "    # Tokenizamos los textos\n",
    "    check = tokenizer(check,prepared)\n",
    "    test = tokenizer(test,prepared)\n",
    "    punct_marks = settings.PUNCT_MARKS if not prepared else settings.PUNCT_MARKS_TOKENS\n",
    "\n",
    "    # Hacemos padding\n",
    "    check, test = padding(check, test)\n",
    "    l_check = len(check)\n",
    "    modifications = []   \n",
    "    \n",
    "    for i in range(l_check):\n",
    "        # Deletions:\n",
    "        # Si test[i] no es un sdp pero check[i] si y las palabras anteriores coinciden salvo mayúsculas\n",
    "        # añadimos la modificación ('D', i) e insertamos el correspondiente sdp faltante en test.\n",
    "        if check[i] in punct_marks:\n",
    "            if test[i] not in punct_marks and test[i-1].upper() == check[i - 1].upper():\n",
    "                modifications.append(('D',i))\n",
    "                test.insert(i,check[i])\n",
    "                \n",
    "        # Reestablecemos el padding para mantener misma longitud\n",
    "        check, test = padding(check, test)\n",
    "\n",
    "        # Insertions:\n",
    "        # Si test[i] es un sdp pero check[i] no y las palabras anteriores coinciden salvo mayúsculas\n",
    "        # añadimos la modificación ('I', i) y eliminamos el correspondiente sdp en test.\n",
    "        if check[i] not in punct_marks:\n",
    "            if test[i] in punct_marks and test[i-1].upper() == check[i - 1].upper():\n",
    "                modifications.append(('I',i))\n",
    "                test.pop(i)\n",
    "                \n",
    "    # Reestablecemos el padding para mantener misma longitud\n",
    "    check, test = padding(check, test)\n",
    "    \n",
    "    # Substitutions:\n",
    "    # Tras haber transformado test para añadir los as sustituciones son aquellos elementos que no coinciden\n",
    "    for i in range(l_check):\n",
    "        if check[i] != test[i] and check[i] != '' and test[i] != '':\n",
    "            modifications.append(('S',i))\n",
    "    return set(modifications)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80814bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import verifyPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fbedc",
   "metadata": {},
   "source": [
    "Utilicemos el ejemplo del documento para ver si funciona correctamente la función. Además verificamos que intercambiar check y test devuelve el mismo número de elementos (distancia de Levenshtein)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f21d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check vs test:  {('D', 1), ('S', 2), ('I', 4), ('S', 5)}\n",
      "test vs check:  {('S', 1), ('I', 1), ('D', 3), ('S', 5)}\n"
     ]
    }
   ],
   "source": [
    "check_example = \"Hello. What's your name?\"\n",
    "test_example = \"Hello what's your, name?\"\n",
    "print('check vs test: ', verifyPunctuation(check_example,test_example))\n",
    "print('test vs check: ', verifyPunctuation(test_example,check_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c3077",
   "metadata": {},
   "source": [
    "## APARTADO 3: Evaluación de modelos.\n",
    "\n",
    "Implementaremos una herramienta que permita recorrer todo el corpus de test y verificación. Es decir, irá recorriendo una a una las líneas de cada fichero (que están alineadas), aplicaría sobre la frase de test el algoritmo básico de puntuación (apartado 1: *addPunctuationBasic* ) y a continuación comprobaría si el resultado es o no correcto usando la función *verifyPunctuation* del apartado 2.\n",
    "\n",
    "En primer lugar vamos a definir una función *evaluate_example* que calcula las métricas precision y recall dado una instancia (check,test). \n",
    "\n",
    "La idea tras esta función es usar *verifyPunctuation* para comparar test y check (cambios necesarios), predicho y test (cambios realizados por el modelo) y entre check y predicho (errores y diferencias). A partir de estos cambios podemos calcular la precisión y el recall como:\n",
    "\n",
    " * precision: número de modificaciones correctas / número de modificaciones hechas\n",
    " * recall: número de modificaciones correctas / número de modificaciones necesarias\n",
    "\n",
    "En la definición de esta función nos encontramos con el problema de que al basarnos en *verifyPunctuation*, que solo devuelve el tipo de error y su posición, se puede considerar como correcto el haber insertado un signo de puntuación que en el check corresponde a otro (Ej: el modelo añade un '.' y debería haber añadido '?'). Al no tener información sobre el caracter, esto podría dar lugar a un falso positivo. Para solucionar esto añadimos un factor de error basado en comparar si \n",
    "en la verificación de la predicción con el check (diferencias) aparecen operaciones distintas, esto es, se ha insertado un signo incorrecto (I) ya que en la verificación aparece una sustitución de ese signo (S).\n",
    "\n",
    "```\n",
    "def evaluate_example(punctuationFunction, check, test,model = None, add_punct_basic = False, print_info = True, prepared = False):\n",
    "    ...\n",
    "    # Diferencia entre el check y el test original (cambios necesarios)\n",
    "    vct = verifyPunctuation(test,check,prepared)\n",
    "    # Diferencia entre el test original y el modificado (modificaciones hechas por la función)\n",
    "    vtmt = verifyPunctuation(test,punctuated,prepared)\n",
    "    # Diferencia entre el check y el modificado\n",
    "    vcmt = verifyPunctuation(punctuated,check,prepared)\n",
    "    es_correcto = 1 if len(vcmt) == 0 else 0\n",
    "    # número de cambios hechos por la función            \n",
    "    hechos_ = len(vtmt)\n",
    "    # número de cambios necesarios\n",
    "    necesarios_ = len(vct)\n",
    "    # número de cambios correctos hechos (intersección entre cambios hechos y necesarios)\n",
    "    correctos_ = len(vtmt & vct) \n",
    "    \n",
    "    # CORRECCIÓN DE ERROR DE SUSTITUCIÓN DE SIGNO\n",
    "    # Esta definición de correctos_ es incompleta bajo el uso de verifyPunctuation ya que por ejemplo si\n",
    "    # el modelo añade al final (token 23 por ejemplo) un '.' y debía añadir '?', en ambos casos recibiremos\n",
    "    # ('I',23) y sin embargo es incorrecto. En resumen, verifyPunctuation no nos devuelve información sobre el \n",
    "    # token en si. Corregimos esto con un factor de error.\n",
    "    error_ = 0\n",
    "    check_tokens = tokenizer(check,prepared)\n",
    "    l_t = len(tokenizer(test,prepared))\n",
    "    l_c = len(check_tokens)\n",
    "    cambios = list(vtmt)\n",
    "    diferencias = list(vcmt)\n",
    "    # Para cada token que aparece en la verificación de la predicción con el test (cambios), comprobamos si \n",
    "    # en la verificación de la predicción con el check (diferencias) aparecen operaciones distintas, esto es,\n",
    "    # se ha insertado un signo incorrecto (I) ya que en la verificación aparece una sustitución de ese signo (S)\n",
    "    for j in range(len(cambios)):\n",
    "        token = cambios[j][1]\n",
    "        operacion = cambios[j][0]\n",
    "        for k in range(len(diferencias)):\n",
    "            error_ += 1 if token == diferencias[k][1] and token<l_t and diferencias[k][1]<l_c and operacion == 'I' and \n",
    "            diferencias[k][0] == 'S' else 0\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6771be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bf505",
   "metadata": {},
   "source": [
    "Comprobamos el funcionamiento de la función con el siguiente ejemplo.\n",
    "\n",
    "* check = \"Hello. What's your name?\"\n",
    "* test = \"hello what's your name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a7de147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LINE: \n",
      "  hello what's your name\n",
      "MODEL PUNCTUATED LINE: \n",
      "  Hello what's your name.\n",
      "VALIDATION LINE: \n",
      "  Hello. What's your name? \n",
      "\n",
      "Modificaciones necesarias:  {('S', 1), ('I', 1), ('I', 4), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('I', 4), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  {('S', 1), ('I', 1), ('S', 4)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  2\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  4 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.25 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0.5, 0.25, 2, 1, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_example = \"Hello. What's your name?\"\n",
    "test_example = \"hello what's your name\"\n",
    "evaluate_example(addPunctuationBasic,check_example,test_example,print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c460dae",
   "metadata": {},
   "source": [
    "Notemos que la oración puntuada es \"Hello what's your name.\", por lo que la función de validación va a considerar como correcto el haber insertado en el token 4, lo que se observa como ('I', 4) tanto en las modificaciones necesarias como las hechas por el modelo. Al no tener información sobre el caracter, esto podría dar lugar a un falso positivo, que es corregido correctamente con el error de sustitución de signo (ver función).\n",
    "\n",
    "Definamos ahora la función principal de este apartado, *evaluate*, que raliza el proceso anterior para distintos corpus y calcula métricas gobales  (sobre todas las intancias) y métricas medias (medias de las métricas de cada instancia evaluada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42740c1b",
   "metadata": {},
   "source": [
    "Evaluamos la función *addPunctuationBasic*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e40008f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.9482794650320423\n",
      "recall global:  0.42824561955393375\n",
      "F1 global:  0.5900314226893488\n",
      "=============================================\n",
      "precision media:  0.9475733555833681\n",
      "recall medio:  0.587400667208851\n",
      "F1 medio:  0.7252308026509773\n",
      "=============================================\n",
      "rendimiento:  0.263384786538729\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_global': 0.9482794650320423,\n",
       " 'recall_global': 0.42824561955393375,\n",
       " 'F1_global': 0.5900314226893488,\n",
       " 'precision_mean': 0.9475733555833681,\n",
       " 'recall_mean': 0.587400667208851,\n",
       " 'F1_mean': 0.7252308026509773,\n",
       " 'score': 0.263384786538729}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(addPunctuationBasic, check_file_path=CHECK_RAW_PATH, test_file_path=TEST_RAW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cd307",
   "metadata": {},
   "source": [
    "Vemos que la precisión es alta, lo que indica que lo que tiene que hacer el modelo (poner la primera letra en mayúscula y un punto al final), lo hace bien. Sin embargo, el recall es relativamente bajo ya que esos cambios no son suficientes para puntuar correctamente las oraciones, cosa que también se deduce del bajo rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6c1ac",
   "metadata": {},
   "source": [
    "Aunque no se pide, definimos la función evaluate_example_from_corpus que permite ver la información resultante de evaluate_example para un elemento en concreto del corpus dado el número de línea corpus_line. De esta forma podemos explorar y verificar el correcto funcionamiento de las funciones definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edded325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate_example_from_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28b6cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LINE: \n",
      "  people working in these canneries could barely stay there all day because of the smell but you know what they came out saying\n",
      "MODEL PUNCTUATED LINE: \n",
      "  People working in these canneries could barely stay there all day because of the smell but you know what they came out saying.\n",
      "VALIDATION LINE: \n",
      "  People working in these canneries could barely stay there all day because of the smell, but you know what they came out saying? \n",
      "\n",
      "Modificaciones necesarias:  {('I', 23), ('I', 15), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('I', 23), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  {('S', 23), ('I', 15)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  2\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.3333333333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_line = 2\n",
    "evaluate_example_from_corpus(addPunctuationBasic,check_file_path=CHECK_RAW_PATH,test_file_path=TEST_RAW_PATH,\\\n",
    "                             corpus_line=corpus_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614e4d2",
   "metadata": {},
   "source": [
    "## APARTADO 4. Modelo basado en 4-gramas. addPunctuation4gram.\n",
    "\n",
    "Utilizando el corpus de entrenamiento contenido en PunctuationTask.train.en construimos un modelo de lenguaje inspirado en la idea de 4-gramas.\n",
    "\n",
    "Definimos una función auxiliar *ngrams* que dado el string text devuelve la lista de N-gramas.\n",
    "\n",
    "```\n",
    "def ngrams(text,N):\n",
    "    \"\"\"\n",
    "    Devuelve la lista formada por las N-tuplas de cadenas de tamaño N presentes en el string 'text' (Ngramas)\n",
    "\n",
    "    input: \n",
    "        text: string a procesar\n",
    "        N: tamaño de las tuplas\n",
    "\n",
    "    output: \n",
    "        lista de Ngramas de un string string\n",
    "    \"\"\"\n",
    "    text = tokenizer(text)\n",
    "    return [tuple(text[i:i+N]) for i in range(len(text)-N+1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f67d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8720",
   "metadata": {},
   "source": [
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ca0d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said'), ('said', ':'), (':', 'Hello'), ('Hello', '!'), ('!', 'nice'), ('nice', 'to'), ('to', 'meet'), ('meet', 'you'), ('you', '!')]\n",
      "3-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said', ':'), ('said', ':', 'Hello'), (':', 'Hello', '!'), ('Hello', '!', 'nice'), ('!', 'nice', 'to'), ('nice', 'to', 'meet'), ('to', 'meet', 'you'), ('meet', 'you', '!')]\n",
      "4-gramas de la oración:  Sara said: Hello! nice to meet you!\n",
      "\t [('Sara', 'said', ':', 'Hello'), ('said', ':', 'Hello', '!'), (':', 'Hello', '!', 'nice'), ('Hello', '!', 'nice', 'to'), ('!', 'nice', 'to', 'meet'), ('nice', 'to', 'meet', 'you'), ('to', 'meet', 'you', '!')]\n"
     ]
    }
   ],
   "source": [
    "text_example = 'Sara said: Hello! nice to meet you!'\n",
    "\n",
    "# Veamos los Ngramas con N de 2 a 4\n",
    "for n in range(2,5):\n",
    "    print(str(n)+'-gramas de la oración: ', text_example)\n",
    "    print('\\t', ngrams(text_example,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f18e57",
   "metadata": {},
   "source": [
    "Creamos la clase **ModelNgram**, que puede generalizarse fácilmente y que instanciaremos con N=4 para definir el modelo propuesto del apartado 4.\n",
    "\n",
    "Esta clase tiene un método *entrena* para entrenar el modelo a partir de un archivo, un método *predice* que devuelve la operación más probable dada un N-1 tupla. Ademas como atributos cuenta con dos diccionarios:\n",
    "\n",
    " * **counts_dict**: para cada operación, cuenta con un diccionario donde cada clave es un N-1 tupla del corpus y su valor es el número de veces que aparece esa tupla precedida de la operación.\n",
    " * **trained_dict**: cada clave es una N-1 tupla del corpus de entrenamiento y su valor es la operación más probable (la que cuenta con mayorres repeticiones en count_dict)\n",
    " \n",
    " ```\n",
    "     def entrena(self, train_file_path):\n",
    "        \"\"\"\n",
    "        Entrena el modelo dada la ruta al corpus de entrenamiento 'train_file_path'\n",
    "        \"\"\"\n",
    "        # Conjunto de tuplas vistas\n",
    "        tuplas = set()\n",
    "        \n",
    "        # Inicializamos los subdiccionarios de cada operación\n",
    "        self.counts_dict[self.mayus]= {}\n",
    "        self.counts_dict[self.minus]={}\n",
    "        for s in self.punct_marks:\n",
    "            self.counts_dict[s] = {}\n",
    "        \n",
    "        # Recorremos el archivo de entrenamiento\n",
    "        with open(train_file_path, 'r', encoding='utf-8') as train:\n",
    "            train_lines = train.readlines()\n",
    "            for line in train_lines:\n",
    "                # Eliminamos el espacio final y retorno de carro del final de las líneas\n",
    "                line = line.rstrip(' \\n')\n",
    "                # Obtenemos los 4gramas de la línea\n",
    "                ngs = ngrams(line,self.N)\n",
    "                for ng in ngs:\n",
    "                    # obtenemos la operación correspondiente a partir del último elemento del 4grama\n",
    "                    last = ng[-1]\n",
    "                    op = last if last in self.punct_marks else self.mayus if last.isupper() else self.minus \n",
    "                    # Construimos la N-1 tupla  \n",
    "                    tupla =  tuple([x.lower() for x in list(ng)[:-1]]) if len(ng) > 2 else ng[0]\n",
    "                    tuplas.add(tupla)\n",
    "                    # Sumamos una ocurrencia a la entrada correspondiente del diccionario\n",
    "                    self.counts_dict[op][tupla]= self.counts_dict[op].get(tupla,0) + 1\n",
    "        train.close()\n",
    "        # Construimos el diccionario trained_model a partir de counts_dict\n",
    "        for tupla in tuplas:\n",
    "            self.trained_dict[tupla] = self.operacion_mas_probable(tupla)\n",
    "        self.trained = True\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fb5eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ngram import ModelNgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b80dd0",
   "metadata": {},
   "source": [
    "Instanciamos el modelo y lo entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da6e256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4gram = ModelNgram(N=4)\n",
    "model4gram.entrena(train_file_path=TRAIN_RAW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb0657",
   "metadata": {},
   "source": [
    "Consultamos el diccionario de conteo para comprobar la distribución de signos dada la terna ('by','the','way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f57fbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocurrencias con el signo  <mayus>  para la terna  ('by', 'the', 'way') : 0\n",
      "Ocurrencias con el signo  <minus>  para la terna  ('by', 'the', 'way') : 25\n",
      "Ocurrencias con el signo  .  para la terna  ('by', 'the', 'way') : 36\n",
      "Ocurrencias con el signo  ,  para la terna  ('by', 'the', 'way') : 200\n",
      "Ocurrencias con el signo  ;  para la terna  ('by', 'the', 'way') : 1\n",
      "Ocurrencias con el signo  :  para la terna  ('by', 'the', 'way') : 0\n",
      "Ocurrencias con el signo  ?  para la terna  ('by', 'the', 'way') : 1\n",
      "Ocurrencias con el signo  !  para la terna  ('by', 'the', 'way') : 0\n"
     ]
    }
   ],
   "source": [
    "terna = ('by','the','way')\n",
    "for i in model4gram.counts_dict:\n",
    "    print('Ocurrencias con el signo ',i,' para la terna ',terna, ':',model4gram.counts_dict[i].get(terna,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c21126",
   "metadata": {},
   "source": [
    "Se observa que la mayor ocurrencia se da para la coma ',' por lo que la función *predice* del modelo debe devolver dicho caracter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cde1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La operación más probable dada la terna  ('by', 'the', 'way')  es:  ,\n"
     ]
    }
   ],
   "source": [
    "print('La operación más probable dada la terna ',terna, ' es: ',model4gram.predice(terna))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d38999",
   "metadata": {},
   "source": [
    "Igual que definimos el modelo de forma genérica para Ngramas, vamos a definir la función genérica addPunctuationNgram donde la función requerida por el apartado addPunctuation4gram es addPunctuationNgram usando un modelo basado en 4 gramas.\n",
    "\n",
    "Esta función cuenta con un parámetro *add_punc_basic* que añade puntuación básica como la de addPuntuationBasic ya que el modelo de 4 gramas nunca va a poner la primera letra en mayúsculas y puede que deje la oración sin puntuación final.\n",
    "\n",
    "```\n",
    "def addPunctuationNgram(model,example,add_basic_punct = False):\n",
    "    ...\n",
    "    # Comprobamos que el modelo ha sido entrenado\n",
    "    model.check_entrenado()\n",
    "    # Trabajamos con los tokens\n",
    "    tokens = tokenizer(example)\n",
    "    num_tokens = len(tokens)\n",
    "    # Generamos los (N-1)-gramas del texto\n",
    "    N = model.N -1\n",
    "    grams = ngrams(example,N)\n",
    "    added_tokens = 0\n",
    "    for i in range(len(grams)):\n",
    "        # Calculamos el 4-grama predicho\n",
    "        operation = model.predice(grams[i]) if N > 1 else model.predice(grams[i][0])\n",
    "        target_index = i+N+added_tokens\n",
    "        # Transformamos los tokens del texto\n",
    "        if operation == model.mayus and target_index < num_tokens:\n",
    "            tokens[target_index] = change_initial(tokens[target_index], uppercase = True)\n",
    "        if operation == model.minus and target_index < num_tokens:\n",
    "            tokens[target_index] = change_initial(tokens[target_index], uppercase = False)\n",
    "        if operation in model.punct_marks:\n",
    "            added_tokens += 1\n",
    "            num_tokens += 1\n",
    "            tokens.insert(target_index, operation)\n",
    "            if operation in model.mayus_marks and target_index < num_tokens -1:\n",
    "                tokens[target_index+1] = change_initial(tokens[target_index+1], uppercase = True)\n",
    "\n",
    "    # Añadimos los espacios excepto para los signos de puntuación\n",
    "    result = [' ' + x if x not in model.punct_marks else x for x in tokens]\n",
    "    # Reconstruimos el texto predicho\n",
    "    result = ''.join(result)[1:]\n",
    "    \n",
    "    # Puesto que el modelo de 4 gramas dificilmente va a poner la primera letra en mayúscula y es probable que\n",
    "    # deje el final de la oración sin puntuar, añadimos la funcionalidad de addPunctuationBasic como\n",
    "    # parámetro de la función.\n",
    "    if add_basic_punct:\n",
    "        dot = '' if result[-1] in model.punct_marks else '.'\n",
    "        result = change_initial(result + dot,uppercase=True)\n",
    "    return result  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53f7eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punctuator_ngram import addPunctuationNgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91219f40",
   "metadata": {},
   "source": [
    "Podemos definir ahora la función de puntuación addPunctuation4gram simplemente como la ejecución de addPunctuationNgram\n",
    "verificando que el modelo usado usa 4gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2490e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPunctuation4gram(model,example,add_basic_punct = False):\n",
    "    N = model.N\n",
    "    assert N == 4, 'The model is based in {} -grams and it should be 4-grams.'.format(str(N))\n",
    "    return addPunctuationNgram(model,example,add_basic_punct = add_basic_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1944aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase sin puntuar:\n",
      " \t and we also are eating meat that comes from some of these same places\n",
      "Puntuación de la frase con modelo 4 gramas:\n",
      " \t and we also are eating meat, that comes from some of these same places\n",
      "Puntuación de la frase con modelo 4 gramas + addPunctuationBasic:\n",
      " \t And we also are eating meat, that comes from some of these same places.\n"
     ]
    }
   ],
   "source": [
    "text_example = \"and we also are eating meat that comes from some of these same places\"\n",
    "print('Frase sin puntuar:\\n \\t',text_example)\n",
    "print('Puntuación de la frase con modelo 4 gramas:\\n \\t',addPunctuation4gram(model4gram,text_example,add_basic_punct=False))\n",
    "print('Puntuación de la frase con modelo 4 gramas + addPunctuationBasic:\\n \\t',addPunctuation4gram(model4gram,text_example,add_basic_punct=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8975744",
   "metadata": {},
   "source": [
    "Exploremos algunos ejemplos usando puntuación básica y sin ella en el modelo de 4gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957ba0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODELO 4GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  it can be a very complicated thing, the ocean\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('I', 7)}\n",
      "Diferencias entre modelo y validación:  {('I', 10), ('S', 0)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  1\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  1\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  0.3333333333333333 \n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "4GRAMS + PUNTUACION BÁSICA\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing, the ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  set() \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  3\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  3\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  1.0 \n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Instancia del corpus. Modificar para ver distintos ejemplos.\n",
    "i = 0\n",
    "\n",
    "l = 70\n",
    "print('='*l)\n",
    "print('MODELO 4GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuation4gram, model = model4gram,test_file_path=TEST_RAW_PATH,\\\n",
    "                             check_file_path= CHECK_RAW_PATH, add_punct_basic=False,corpus_line = i)\n",
    "print('='*l)\n",
    "print('='*l)\n",
    "print('4GRAMS + PUNTUACION BÁSICA')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuation4gram, model = model4gram, test_file_path=TEST_RAW_PATH,\\\n",
    "                             check_file_path= CHECK_RAW_PATH,add_punct_basic=True,corpus_line = i)\n",
    "print('='*l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56a6cb",
   "metadata": {},
   "source": [
    "## APARTADO 5: Evaluación de addPunctuation4gram y comparación de modelos.\n",
    "\n",
    "Evaluemos el modelo usando la misma función *evaluate* anterior. Vamos a comparar el rendimiento usando también la puntuación básica y sin ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c01512bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELO 4GRAMS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.3061919504643963\n",
      "recall global:  0.04666708609896505\n",
      "F1 global:  0.08099033684555332\n",
      "=============================================\n",
      "precision media:  0.14064391487397296\n",
      "recall medio:  0.04956605113889393\n",
      "F1 medio:  0.07329966587077798\n",
      "=============================================\n",
      "rendimiento:  0.0\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MODELO 4GRAMS')\n",
    "evaluate(addPunctuation4gram,model=model4gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c333949",
   "metadata": {},
   "source": [
    "Podemos ver que este modelo tal cual es muy pobre. Usando la función *evaluate_example_from_corpus*, podemos ver que nunca pone la mayúscula inicial como era de esperar y rara vez un punto al final. Además las predicciones parecen bastante pobres y esto se puede deber a la falta de variedad en en las tuplas del corpus de entrenamiento. Una posible mejoría se conseguiría con un corpus más grande y variado.\n",
    "\n",
    "Veamos la evaluación añadiendo la puntuación básica y lo comparamos con lo obtenido en addPunctuationBasic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4876526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4GRAMS + PUNTUACION BÁSICA\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.7801183367117734\n",
      "recall global:  0.447937965963069\n",
      "F1 global:  0.569101954358339\n",
      "=============================================\n",
      "precision media:  0.8433161902915692\n",
      "recall medio:  0.5991575233871246\n",
      "F1 medio:  0.7005732377871406\n",
      "=============================================\n",
      "rendimiento:  0.2369628702544848\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n",
      "PUNTUACIÓN BÁSICA\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.9482794650320423\n",
      "recall global:  0.42824561955393375\n",
      "F1 global:  0.5900314226893488\n",
      "=============================================\n",
      "precision media:  0.9475733555833681\n",
      "recall medio:  0.587400667208851\n",
      "F1 medio:  0.7252308026509773\n",
      "=============================================\n",
      "rendimiento:  0.263384786538729\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('4GRAMS + PUNTUACION BÁSICA')\n",
    "evaluate(addPunctuation4gram,model=model4gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,\\\n",
    "         add_punct_basic=True)\n",
    "print()\n",
    "print('PUNTUACIÓN BÁSICA')\n",
    "evaluate(addPunctuationBasic,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ee6ed",
   "metadata": {},
   "source": [
    "Podemos ver que en general la precisión es más baja en el modelo de 4gramas pero el recall ligeramente superior, pero no de forma sustancial. Además los F1 son menores en el modelo de puntuación básica que en el de 4 gramas así como el rendimiento.\n",
    "\n",
    "En general podemos concluir que el modelo de puntuación básica es ligeramente mejor que el basado en 4gramas cuando se añade la puntuación línea a línea por sorprendente que parezca. Esto puede deverse a que muchas de las oraciones del corpus de entrenamiento no presentan signos intermedios (sólo mayúscula inicial y punto final, exactamente lo que hace addPunctuationBasic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02447dfb",
   "metadata": {},
   "source": [
    "Ya que hemos implementado un modelo genérico, vamos a comparar con modelos basados en 3gramas y 5gramas para ver si existe mejoría o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df5a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3gram = ModelNgram(N=3)\n",
    "model5gram = ModelNgram(N=5)\n",
    "model3gram.entrena(train_file_path=TRAIN_RAW_PATH)\n",
    "model5gram.entrena(train_file_path=TRAIN_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c0b5296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3GRAMAS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.7269191453289785\n",
      "recall global:  0.45858630343829626\n",
      "F1 global:  0.5623848698486792\n",
      "=============================================\n",
      "precision media:  0.8041213477542051\n",
      "recall medio:  0.6055814495149251\n",
      "F1 medio:  0.6908704051694091\n",
      "=============================================\n",
      "rendimiento:  0.2259769155889306\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_global': 0.7269191453289785,\n",
       " 'recall_global': 0.45858630343829626,\n",
       " 'F1_global': 0.5623848698486792,\n",
       " 'precision_mean': 0.8041213477542051,\n",
       " 'recall_mean': 0.6055814495149251,\n",
       " 'F1_mean': 0.6908704051694091,\n",
       " 'score': 0.2259769155889306}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3GRAMAS')\n",
    "evaluate(addPunctuationNgram, model = model3gram,test_file_path=TEST_RAW_PATH,\\\n",
    "         check_file_path= CHECK_RAW_PATH, add_punct_basic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b82155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5GRAMAS\n",
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.8715371568565865\n",
      "recall global:  0.43644027808361385\n",
      "F1 global:  0.581621530980129\n",
      "=============================================\n",
      "precision media:  0.9022099935324751\n",
      "recall medio:  0.592608753166226\n",
      "F1 medio:  0.7153476507331434\n",
      "=============================================\n",
      "rendimiento:  0.2533027395355305\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('5GRAMAS')\n",
    "evaluate(addPunctuationNgram, model = model5gram, test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH,add_punct_basic=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06a197",
   "metadata": {},
   "source": [
    "Observamos que en cuanto a precision, el modelo basado en 3gramas es ligeramente inferior al de 4gramas y este a su vez inferior al de 5 gramas, todos por debajo de la precisión del puntuador básico. En cuanto a recall el orden es inverso: el modelo basado en 3 gramas presenta mayor recall que el de 4 y este a su vez que el de 5.\n",
    "En cuanto al F1, los mayores valores los encontramos para el modelo de 5gramas y de puntuación básica. Es obvio que cuanto mayor sea N, menos probable es la probabilidad de que se de una N-tupla concreta por lo que el modelo basado en N-gramas con N grande coincidirá eventualmente con el de puntuación básica si se considera el parámetro add_punct_basic.\n",
    "\n",
    "Para acabar este apartado podemos explorar algunas predicciones de los modelos de 3 y 5 gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a10a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODELO 3GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing. The ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('S', 7), ('I', 7), ('I', 9), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  {('S', 7), ('S', 8)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  4\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  2\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.5\n",
      "recall (n_correctas/n_necesarias):  0.6666666666666666 \n",
      "\n",
      "======================================================================\n",
      "MODELO 5GRAMS\n",
      "======================================================================\n",
      "TEST LINE: \n",
      "  it can be a very complicated thing the ocean\n",
      "MODEL PUNCTUATED LINE: \n",
      "  It can be a very complicated thing, the ocean.\n",
      "VALIDATION LINE: \n",
      "  It can be a very complicated thing, the ocean. \n",
      "\n",
      "Modificaciones necesarias:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('I', 7), ('I', 9), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  set() \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  3\n",
      "n_correctas (Núm. de modificaciones correctas, i.e., \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  3\n",
      "n_necesarias (Núm. de modificaciones necesarias):  3 \n",
      "\n",
      "precision (n_correctas/n_hechas):  1.0\n",
      "recall (n_correctas/n_necesarias):  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instancia del corpus. Modificar para ver distintos ejemplos.\n",
    "i= 0\n",
    "\n",
    "l = 70 \n",
    "print('='*l)\n",
    "print('MODELO 3GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuationNgram, model = model3gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH, add_punct_basic=True,corpus_line = i)\n",
    "print('='*l)\n",
    "print('MODELO 5GRAMS')\n",
    "print('='*l)\n",
    "evaluate_example_from_corpus(addPunctuationNgram, model = model5gram,test_file_path=TEST_RAW_PATH, check_file_path= CHECK_RAW_PATH, add_punct_basic=True,corpus_line = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5852e71",
   "metadata": {},
   "source": [
    "# Parte 2. Apartado 6.\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Utilizando también ejemplos de TED talks, en un artículo de 2016, Ottokar Tilk y Tanel Alum ha aplicado un modelo de redes recurrentes bidireccionales para esta misma tarea (restauración de signos de puntuación en textos no segmentados).\n",
    "El artículo donde lo describen se encuentra publicado en este enlace:\n",
    " * [Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration (InterSpeech 2016)](https://www.isca-speech.org/archive_v0/Interspeech_2016/pdfs/1517.PDF)\n",
    "\n",
    "El código correspondiente a su implementación se encuentra también disponible en github:\n",
    " * https://github.com/ottokart/punctuator2\n",
    "\n",
    "En este apartado vamos a estudiar la implementación de los autores y adaptar su trabajo al escenario de este ejercicio, entrenando el modelo y realizando la puntuacion del archivo PunctuationTask.test.en, para luego evaluarlo usando el método tanto de los autores como la función *evaluate* implementada en el la parte 1.\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Los autores presentan un modelo de red neuronal recurrente bidireccional con mecanismo de atención para la restauración de la puntuación en textos no segmentados. El modelo puede utilizar contextos largos en ambas direcciones y dirigir la atención cuando sea necesario, lo que le permite superar el estado del arte anterior en los conjuntos de datos de inglés (IWSLT2011) y estonio por un amplio margen.\n",
    "\n",
    "El modelo es una red neuronal recurrente bidireccional (BRNN) que le permite hacer uso de contextos de longitud no fija antes y después de la posición actual en el texto. En las capas recurrentes se utilizan unidades recurrentes cerradas (GRU) que son muy adecuadas para capturar dependencias de largo alcance en múltiples escalas de tiempo. Estas unidades tienen beneficios similares a los de las unidades LSTM, pero son más simples. Incorporan un mecanismo de atención  al modelo para aumentar su capacidad de encontrar partes relevantes del contexto para las decisiones de puntuación. Por ejemplo, el modelo podría centrarse en palabras que indican una pregunta, pero que pueden estar relativamente lejos de la palabra actual, para empujar al modelo a terminar la frase con un signo de interrogación en lugar de un punto. \n",
    "\n",
    "En la implementación , los signos de puntuación se encuentran tokenizados, esto es, aparecen como:\n",
    "   * \".\" -> .PERIOD\n",
    "   * \",\" -> ,COMMA\n",
    "   * \";\" -> ;SEMICOLON\n",
    "   * \":\" -> :COLON\n",
    "   * \"?\" -> ?QUESTIONMARK\n",
    "   * \"!\" -> !EXCLAMATIONMARK\n",
    "\n",
    "Sin embargo, el input del modelo requiere de un embedding de texto. Para ello, se disponde de un módulo data.py que vectoriza textos preparados con la tokenización anterior extrayendo el vocabulario de un corpues de entrenamiento.\n",
    "\n",
    "## Implementación\n",
    "\n",
    "El modelo que vamos a usar aquí no es exactamente el del repositorio anterior, que usa Theano y es muy lento a la hora de entrenar y puntuar. Para ello usamos la implementacion en TensorFlow dada por los propios autores en  el siguiente repositorio:\n",
    "\n",
    " * https://github.com/cadia-lvl/punctuation-prediction/tree/master/punctuator2tf2\n",
    "\n",
    "Este modelo se encuentra en el directorio **punctuator2tf2** y sus principales módulos son los siguientes:\n",
    " \n",
    " * **data.py**: vectoriza textos preparados tokenización extrayendo el vocabulario de un corpues de entrenamiento.\n",
    " * **models.py**: definición del modelo.\n",
    " * **main.py**: entrena el modelo y genera un archivo con los parámetros del mismo.\n",
    " * **punctuator.py**: puntúa un archivo de texto dado un modelo preentrenado.\n",
    " * **error_calculator.py**: evalúa untexto puntuado a partir de otro de validación calculando la precisión, recall y F1.\n",
    "\n",
    "## Procesos y modificaciones\n",
    "\n",
    " El proceso que vamos a ver en este documento consta ded las siguientes fases:\n",
    "\n",
    "  1. Preprocesamiento de los datos crudos: usamos funciones que hemos definido en el módulo **preprocessor.py** para transformar los signos en los tokens anteriores, asi como los numéricos por el token \"<NUM\\>\"\n",
    "  1. Generación de archivos preparados: usamod el módulo **data.py** para vectorizar los textos preprocesados.\n",
    "  1. Entrenamos un modelo de 256 capas con learning rate de 0.02 durante 8 épocas con tamaño de minibatch 64.\n",
    "  1. Puntuamos el archivo PunctuationTask.test.en.\n",
    "  1. Evaluamos la puntuación con el módulo **error_calculator.py** de los autores.\n",
    "  1. Evaluamos la puntuación con nuestro módulo **evaluator** basado en *VerrifyPunctuation* definido en la parte 1.\n",
    "\n",
    "Para llevar esto a cabo y mejorar el propio modelo de los autores se ha realizado una modificación en el módulo **punctuator.py**, ya que el modelo original no pone en mayúsculas las palabras tras signos de final de oración. Además se ha añadido puntuación básica (primera letra en mayúsculas y punto al final si no hay un signo de final de oración), pues el modelo suele dejar los finales de oraciones sin puntuar con con signos intermedios (Ej.: comas). \n",
    "\n",
    "Fragmento modificado en **punctuator.py**:\n",
    "```\n",
    "def restore(text, word_vocabulary, reverse_punctuation_vocabulary, model):\n",
    "    ...\n",
    "    # iteramos en los elementos de la secuencia predicha\n",
    "    for j in range(step):\n",
    "       # si no es el final\n",
    "       if j < step - 1:\n",
    "           # añadimos el signo de puntuación correspondiente si no es un espacio\n",
    "           punctuated += \" \" + punctuations[j] + \" \" if punctuations[j] != data.SPACE else \" \"\n",
    "           # AÑADIDO POR JULIÁN: si el signo añadido era EOS, ponemos en mayúsculas la siguiente palabra.\n",
    "           punctuated += subsequence[1+j] if not punctuations[j] in data.EOS_TOKENS  else to_upper(subsequence[1+j])\n",
    "        # AÑADIDO POR JULIÁN: si el último token no es EOS se añade el token de punto (PUNTUACIÓN BÁSICA)\n",
    "        elif j == step -1: punctuated += \" .PERIOD \" if punctuations[j] not in data.EOS_TOKENS else  \" \" + \n",
    "          punctuations[j] + \" \"\n",
    "    if subsequence[-1] == data.END:\n",
    "        break\n",
    "    i += step\n",
    "  # AÑADIDO POR JULIÁN: Se pone en mayúsculas la primera letra (PUNTUACIÓN BÁSICA)\n",
    "  return to_upper(punctuated)\n",
    "\n",
    "```\n",
    "\n",
    "**IMPORTANTE:**\n",
    " * Esta parte del notebook está pensada para ser ejecutado a parte en Google Collab con GPU ya que el modelo tarda mucho en entrenar y puntuar (entorno a 1 hora). Por ello se van a comentar las celdas referentes a esos procesos en caso de que se ejecuten sin querer.\n",
    "\n",
    "**NOTA**: Al igual que en la parte 1, iré cargando las funciones implementadas conforme se vayan explicando, por lo que recomiendo consultar el código de los módulos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2512116",
   "metadata": {},
   "source": [
    "En caso de que se quieran ejecutar las distintas celdas, es posible que haya que instalar las dependencias definidas por los autores. Para ello se ejecuta la siguiente celda comentada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda1c485",
   "metadata": {
    "id": "eov47nuWaY3E"
   },
   "outputs": [],
   "source": [
    "#!pip install -r {settings.PUNCTUATOR2TFT_DIR}/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367d948",
   "metadata": {},
   "source": [
    "Para preparar los archivos (sustituir los signos por tokens), utilizamos la función *prepare_file* definida en **preprocessor**, que realiza transformaciones como la del siguiente ejemplo:\n",
    "\n",
    " * Sin procesar: Yeah, there's 38 million of them sold worldwide. \n",
    " * Procesada: Yeah ,COMMA there's <NUM\\> million of them sold worldwide .PERIOD \n",
    "\n",
    "Esta función se basa en *prepare_text*, del mismo módulo **preprocessor.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f56b5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import prepare_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f66a176b",
   "metadata": {
    "id": "Chx0_37TaUMM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\prepared\\test.prepared.txt\n",
      ".\\data\\prepared\\check.prepared.txt\n",
      ".\\data\\prepared\\train.prepared.txt\n"
     ]
    }
   ],
   "source": [
    "TEST_PREP_PATH  = prepare_file(TEST_RAW_PATH,'test.prepared.txt')\n",
    "CHECK_PREP_PATH = prepare_file(CHECK_RAW_PATH,'check.prepared.txt',allow_duplicates=False)\n",
    "TRAIN_PREP_PATH = prepare_file(TRAIN_RAW_PATH,'train.prepared.txt',lowercase=True)\n",
    "\n",
    "print(TEST_PREP_PATH)\n",
    "print(CHECK_PREP_PATH)\n",
    "print(TRAIN_PREP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f963603",
   "metadata": {},
   "source": [
    "El módulo data.py requiere de varios archivos para poder generar los archivos necesarios para el entrenamiento del modelo. Para ello hemos definido una función 'train_dev_test_split' que separará el archivo de entrenamiento test.prepared.txt en otros tres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e367c39b",
   "metadata": {
    "id": "uBZNEXxpaf_M"
   },
   "outputs": [],
   "source": [
    "from preprocessor import train_dev_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29dee14c",
   "metadata": {
    "id": "HpQ4I5uaag7s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\prepared\\train.split.train.txt\n",
      ".\\data\\prepared\\train.split.dev.txt\n",
      ".\\data\\prepared\\train.split.test.txt\n"
     ]
    }
   ],
   "source": [
    "TRAIN_TRAIN_PREP_PATH, TRAIN_DEV_PREP_PATH, TRAIN_TEST_PREP_PATH = train_dev_test_split(TRAIN_PREP_PATH, train_split = 0.7, dev_split = 0.15)\n",
    "\n",
    "print(TRAIN_TRAIN_PREP_PATH)\n",
    "print(TRAIN_DEV_PREP_PATH)\n",
    "print(TRAIN_TEST_PREP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f18b5",
   "metadata": {},
   "source": [
    "Con estos archivos ya podemos ejecutar el módulo **data.py** del modelo punctuator2tf2. Para ello debemos pasarle el directorio de los archivos anteriores. Ahí busca aquellos que terminan en *train.txt, *test.txt y *dev.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dd4c9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s09u3Xkjai7U",
    "outputId": "3dec046d-71b3-47af-a997-4584ec4615d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45087\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Vocabulary \"data/preprocessed/punctuations\" size: 7\n",
      "0.22% UNK-s in data/preprocessed/train\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Vocabulary \"data/preprocessed/punctuations\" size: 7\n",
      "0.86% UNK-s in data/preprocessed/dev\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Vocabulary \"data/preprocessed/punctuations\" size: 7\n",
      "0.84% UNK-s in data/preprocessed/test\n"
     ]
    }
   ],
   "source": [
    "#!python {settings.PUNCTUATOR2TF2_DIR}/data.py {settings.DATA_PREPARED_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c41c8",
   "metadata": {},
   "source": [
    "Los archivos vectorizados junto a los vocabularios de palabras y puntuación se encuentran en el directorio data/preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5a996ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos disponibles en .\\data\\preprocessed:  ['dev', 'punctuations', 'test', 'train', 'vocabulary']\n"
     ]
    }
   ],
   "source": [
    "print('Archivos disponibles en {}: '.format(settings.DATA_PREPROCESSED_DIR),os.listdir(settings.DATA_PREPROCESSED_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1716207",
   "metadata": {},
   "source": [
    "Ya está todo preparado para entrenar el modelo. Definimos los parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7293ad81",
   "metadata": {
    "id": "SHBheWpGak36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_punctuator_h256_lr0.02.pcl\n"
     ]
    }
   ],
   "source": [
    "model_name = 'punctuator'\n",
    "n_layers = 256\n",
    "lr = 0.02\n",
    "model_full_name = 'Model_{}_h{}_lr{}.pcl'.format(model_name,n_layers,lr)\n",
    "print(model_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfd6c6",
   "metadata": {},
   "source": [
    "Lo entrenamos (tarda entorno a media hora con varias GPUs de alto rendimiento de Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbfae60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffsKQ6xfamR0",
    "outputId": "e2ea223f-1591-436f-92cf-81062fe0755a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 0.02 Model_punctuator_h256_lr0.02.pcl\n",
      "Building model ...\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "2022-06-20 05:59:33.918308: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Vocabulary \"data/preprocessed/punctuations\" size: 7\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "Total number of trainable parameters: 13579271\n",
      "Training...\n",
      "PPL: 1.9916; Speed: 14456.22 sps\n",
      "PPL: 1.6755; Speed: 14837.38 sps\n",
      "PPL: 1.5495; Speed: 14976.21 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 3385.17431640625\n",
      "Total num output samples in dev iteration 323, epoch 0: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 6377.9140625\n",
      "Total num output samples in dev iteration 323, epoch 0: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 9627.173828125\n",
      "Total num output samples in dev iteration 323, epoch 0: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 12884.37890625\n",
      "Total num output samples in dev iteration 323, epoch 0: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 16229.13671875\n",
      "Total num output samples in dev iteration 323, epoch 0: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 19253.966796875\n",
      "Total num output samples in dev iteration 323, epoch 0: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 22396.177734375\n",
      "Total num output samples in dev iteration 323, epoch 0: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 25501.568359375\n",
      "Total num output samples in dev iteration 323, epoch 0: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 28620.34765625\n",
      "Total num output samples in dev iteration 323, epoch 0: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 31575.4453125\n",
      "Total num output samples in dev iteration 323, epoch 0: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 35206.3046875\n",
      "Total num output samples in dev iteration 323, epoch 0: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 38819.27734375\n",
      "Total num output samples in dev iteration 323, epoch 0: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 42176.00390625\n",
      "Total num output samples in dev iteration 323, epoch 0: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 45394.99609375\n",
      "Total num output samples in dev iteration 323, epoch 0: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 49479.8515625\n",
      "Total num output samples in dev iteration 323, epoch 0: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 53002.37109375\n",
      "Total num output samples in dev iteration 323, epoch 0: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 56334.03125\n",
      "Total num output samples in dev iteration 323, epoch 0: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 59991.546875\n",
      "Total num output samples in dev iteration 323, epoch 0: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 63296.67578125\n",
      "Total num output samples in dev iteration 323, epoch 0: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 66601.734375\n",
      "Total num output samples in dev iteration 323, epoch 0: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 69684.2109375\n",
      "Total num output samples in dev iteration 323, epoch 0: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 73009.8984375\n",
      "Total num output samples in dev iteration 323, epoch 0: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 76261.0234375\n",
      "Total num output samples in dev iteration 323, epoch 0: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 79640.3984375\n",
      "Total num output samples in dev iteration 323, epoch 0: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 82742.921875\n",
      "Total num output samples in dev iteration 323, epoch 0: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 86026.296875\n",
      "Total num output samples in dev iteration 323, epoch 0: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 0: 89269.359375\n",
      "Total num output samples in dev iteration 323, epoch 0: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.2963999509811401\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.2750; Speed: 15154.42 sps\n",
      "PPL: 1.2652; Speed: 15204.02 sps\n",
      "PPL: 1.2563; Speed: 15225.68 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 2611.953125\n",
      "Total num output samples in dev iteration 323, epoch 1: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 4975.78466796875\n",
      "Total num output samples in dev iteration 323, epoch 1: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 7508.17578125\n",
      "Total num output samples in dev iteration 323, epoch 1: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 10072.677734375\n",
      "Total num output samples in dev iteration 323, epoch 1: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 12639.939453125\n",
      "Total num output samples in dev iteration 323, epoch 1: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 14964.54296875\n",
      "Total num output samples in dev iteration 323, epoch 1: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 17403.63671875\n",
      "Total num output samples in dev iteration 323, epoch 1: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 19783.166015625\n",
      "Total num output samples in dev iteration 323, epoch 1: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 22295.859375\n",
      "Total num output samples in dev iteration 323, epoch 1: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 24634.359375\n",
      "Total num output samples in dev iteration 323, epoch 1: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 27422.095703125\n",
      "Total num output samples in dev iteration 323, epoch 1: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 30189.287109375\n",
      "Total num output samples in dev iteration 323, epoch 1: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 32835.140625\n",
      "Total num output samples in dev iteration 323, epoch 1: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 35333.1171875\n",
      "Total num output samples in dev iteration 323, epoch 1: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 38606.83203125\n",
      "Total num output samples in dev iteration 323, epoch 1: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 41376.73046875\n",
      "Total num output samples in dev iteration 323, epoch 1: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 43972.2578125\n",
      "Total num output samples in dev iteration 323, epoch 1: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 47015.0859375\n",
      "Total num output samples in dev iteration 323, epoch 1: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 49667.3515625\n",
      "Total num output samples in dev iteration 323, epoch 1: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 52220.95703125\n",
      "Total num output samples in dev iteration 323, epoch 1: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 54594.12890625\n",
      "Total num output samples in dev iteration 323, epoch 1: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 57228.3046875\n",
      "Total num output samples in dev iteration 323, epoch 1: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 59855.18359375\n",
      "Total num output samples in dev iteration 323, epoch 1: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 62538.375\n",
      "Total num output samples in dev iteration 323, epoch 1: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 64977.1171875\n",
      "Total num output samples in dev iteration 323, epoch 1: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 67562.0390625\n",
      "Total num output samples in dev iteration 323, epoch 1: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 1: 70141.25\n",
      "Total num output samples in dev iteration 323, epoch 1: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.2263000011444092\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.2045; Speed: 15150.01 sps\n",
      "PPL: 1.2014; Speed: 15196.61 sps\n",
      "PPL: 1.1977; Speed: 15220.71 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 2229.62158203125\n",
      "Total num output samples in dev iteration 323, epoch 2: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 4278.7353515625\n",
      "Total num output samples in dev iteration 323, epoch 2: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 6425.8076171875\n",
      "Total num output samples in dev iteration 323, epoch 2: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 8600.3486328125\n",
      "Total num output samples in dev iteration 323, epoch 2: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 10796.23046875\n",
      "Total num output samples in dev iteration 323, epoch 2: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 12758.75390625\n",
      "Total num output samples in dev iteration 323, epoch 2: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 14925.78515625\n",
      "Total num output samples in dev iteration 323, epoch 2: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 16951.140625\n",
      "Total num output samples in dev iteration 323, epoch 2: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 19061.11328125\n",
      "Total num output samples in dev iteration 323, epoch 2: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 21093.23828125\n",
      "Total num output samples in dev iteration 323, epoch 2: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 23477.8984375\n",
      "Total num output samples in dev iteration 323, epoch 2: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 25872.890625\n",
      "Total num output samples in dev iteration 323, epoch 2: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 28190.775390625\n",
      "Total num output samples in dev iteration 323, epoch 2: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 30405.009765625\n",
      "Total num output samples in dev iteration 323, epoch 2: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 33223.08984375\n",
      "Total num output samples in dev iteration 323, epoch 2: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 35605.98828125\n",
      "Total num output samples in dev iteration 323, epoch 2: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 37883.19140625\n",
      "Total num output samples in dev iteration 323, epoch 2: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 40514.15625\n",
      "Total num output samples in dev iteration 323, epoch 2: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 42787.265625\n",
      "Total num output samples in dev iteration 323, epoch 2: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 44965.61328125\n",
      "Total num output samples in dev iteration 323, epoch 2: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 46951.7421875\n",
      "Total num output samples in dev iteration 323, epoch 2: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 49183.83984375\n",
      "Total num output samples in dev iteration 323, epoch 2: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 51516.4453125\n",
      "Total num output samples in dev iteration 323, epoch 2: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 53832.97265625\n",
      "Total num output samples in dev iteration 323, epoch 2: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 55972.17578125\n",
      "Total num output samples in dev iteration 323, epoch 2: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 58180.24609375\n",
      "Total num output samples in dev iteration 323, epoch 2: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 2: 60377.1640625\n",
      "Total num output samples in dev iteration 323, epoch 2: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.1919000148773193\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.1611; Speed: 15159.31 sps\n",
      "PPL: 1.1593; Speed: 15209.20 sps\n",
      "PPL: 1.1576; Speed: 15222.37 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 1936.87060546875\n",
      "Total num output samples in dev iteration 323, epoch 3: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 3608.28857421875\n",
      "Total num output samples in dev iteration 323, epoch 3: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 5537.85986328125\n",
      "Total num output samples in dev iteration 323, epoch 3: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 7371.5830078125\n",
      "Total num output samples in dev iteration 323, epoch 3: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 9225.818359375\n",
      "Total num output samples in dev iteration 323, epoch 3: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 10848.025390625\n",
      "Total num output samples in dev iteration 323, epoch 3: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 12723.287109375\n",
      "Total num output samples in dev iteration 323, epoch 3: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 14391.0576171875\n",
      "Total num output samples in dev iteration 323, epoch 3: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 16198.31640625\n",
      "Total num output samples in dev iteration 323, epoch 3: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 17936.453125\n",
      "Total num output samples in dev iteration 323, epoch 3: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 19973.4140625\n",
      "Total num output samples in dev iteration 323, epoch 3: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 22027.42578125\n",
      "Total num output samples in dev iteration 323, epoch 3: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 23921.857421875\n",
      "Total num output samples in dev iteration 323, epoch 3: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 25874.552734375\n",
      "Total num output samples in dev iteration 323, epoch 3: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 28229.595703125\n",
      "Total num output samples in dev iteration 323, epoch 3: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 30319.240234375\n",
      "Total num output samples in dev iteration 323, epoch 3: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 32265.09375\n",
      "Total num output samples in dev iteration 323, epoch 3: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 34604.27734375\n",
      "Total num output samples in dev iteration 323, epoch 3: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 36540.72265625\n",
      "Total num output samples in dev iteration 323, epoch 3: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 38426.75\n",
      "Total num output samples in dev iteration 323, epoch 3: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 40133.45703125\n",
      "Total num output samples in dev iteration 323, epoch 3: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 42110.52734375\n",
      "Total num output samples in dev iteration 323, epoch 3: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 44187.81640625\n",
      "Total num output samples in dev iteration 323, epoch 3: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 46172.21875\n",
      "Total num output samples in dev iteration 323, epoch 3: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 47923.3515625\n",
      "Total num output samples in dev iteration 323, epoch 3: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 49881.05078125\n",
      "Total num output samples in dev iteration 323, epoch 3: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 3: 51768.515625\n",
      "Total num output samples in dev iteration 323, epoch 3: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.162500023841858\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.1265; Speed: 15201.70 sps\n",
      "PPL: 1.1260; Speed: 15236.01 sps\n",
      "PPL: 1.1251; Speed: 15246.19 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 1671.848876953125\n",
      "Total num output samples in dev iteration 323, epoch 4: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 3220.705322265625\n",
      "Total num output samples in dev iteration 323, epoch 4: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 4880.056640625\n",
      "Total num output samples in dev iteration 323, epoch 4: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 6490.203125\n",
      "Total num output samples in dev iteration 323, epoch 4: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 8080.03515625\n",
      "Total num output samples in dev iteration 323, epoch 4: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 9470.548828125\n",
      "Total num output samples in dev iteration 323, epoch 4: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 11145.31640625\n",
      "Total num output samples in dev iteration 323, epoch 4: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 12641.3193359375\n",
      "Total num output samples in dev iteration 323, epoch 4: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 14240.3681640625\n",
      "Total num output samples in dev iteration 323, epoch 4: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 15738.458984375\n",
      "Total num output samples in dev iteration 323, epoch 4: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 17530.916015625\n",
      "Total num output samples in dev iteration 323, epoch 4: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 19419.849609375\n",
      "Total num output samples in dev iteration 323, epoch 4: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 21116.955078125\n",
      "Total num output samples in dev iteration 323, epoch 4: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 22866.044921875\n",
      "Total num output samples in dev iteration 323, epoch 4: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 24905.708984375\n",
      "Total num output samples in dev iteration 323, epoch 4: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 26719.337890625\n",
      "Total num output samples in dev iteration 323, epoch 4: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 28448.6484375\n",
      "Total num output samples in dev iteration 323, epoch 4: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 30434.787109375\n",
      "Total num output samples in dev iteration 323, epoch 4: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 32092.603515625\n",
      "Total num output samples in dev iteration 323, epoch 4: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 33690.1953125\n",
      "Total num output samples in dev iteration 323, epoch 4: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 35148.046875\n",
      "Total num output samples in dev iteration 323, epoch 4: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 36829.2734375\n",
      "Total num output samples in dev iteration 323, epoch 4: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 38650.4609375\n",
      "Total num output samples in dev iteration 323, epoch 4: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 40348.5546875\n",
      "Total num output samples in dev iteration 323, epoch 4: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 41893.89453125\n",
      "Total num output samples in dev iteration 323, epoch 4: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 43660.8828125\n",
      "Total num output samples in dev iteration 323, epoch 4: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 4: 45283.328125\n",
      "Total num output samples in dev iteration 323, epoch 4: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.1407999992370605\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.0992; Speed: 15157.53 sps\n",
      "PPL: 1.0987; Speed: 15205.19 sps\n",
      "PPL: 1.0983; Speed: 15230.16 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 1533.228515625\n",
      "Total num output samples in dev iteration 323, epoch 5: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 2851.708251953125\n",
      "Total num output samples in dev iteration 323, epoch 5: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 4313.78271484375\n",
      "Total num output samples in dev iteration 323, epoch 5: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 5617.04296875\n",
      "Total num output samples in dev iteration 323, epoch 5: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 7008.6015625\n",
      "Total num output samples in dev iteration 323, epoch 5: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 8216.052734375\n",
      "Total num output samples in dev iteration 323, epoch 5: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 9689.3486328125\n",
      "Total num output samples in dev iteration 323, epoch 5: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 11010.2021484375\n",
      "Total num output samples in dev iteration 323, epoch 5: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 12368.9951171875\n",
      "Total num output samples in dev iteration 323, epoch 5: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 13643.8828125\n",
      "Total num output samples in dev iteration 323, epoch 5: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 15185.41796875\n",
      "Total num output samples in dev iteration 323, epoch 5: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 16811.9453125\n",
      "Total num output samples in dev iteration 323, epoch 5: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 18313.60546875\n",
      "Total num output samples in dev iteration 323, epoch 5: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 19852.060546875\n",
      "Total num output samples in dev iteration 323, epoch 5: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 21517.736328125\n",
      "Total num output samples in dev iteration 323, epoch 5: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 23095.583984375\n",
      "Total num output samples in dev iteration 323, epoch 5: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 24567.8671875\n",
      "Total num output samples in dev iteration 323, epoch 5: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 26353.34375\n",
      "Total num output samples in dev iteration 323, epoch 5: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 27725.3203125\n",
      "Total num output samples in dev iteration 323, epoch 5: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 29053.44921875\n",
      "Total num output samples in dev iteration 323, epoch 5: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 30259.873046875\n",
      "Total num output samples in dev iteration 323, epoch 5: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 31674.609375\n",
      "Total num output samples in dev iteration 323, epoch 5: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 33291.9765625\n",
      "Total num output samples in dev iteration 323, epoch 5: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 34801.73828125\n",
      "Total num output samples in dev iteration 323, epoch 5: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 36131.2734375\n",
      "Total num output samples in dev iteration 323, epoch 5: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 37654.828125\n",
      "Total num output samples in dev iteration 323, epoch 5: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 5: 39057.3984375\n",
      "Total num output samples in dev iteration 323, epoch 5: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.120300054550171\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.0761; Speed: 15180.55 sps\n",
      "PPL: 1.0764; Speed: 15220.63 sps\n",
      "PPL: 1.0767; Speed: 15246.80 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 1339.527587890625\n",
      "Total num output samples in dev iteration 323, epoch 6: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 2467.34521484375\n",
      "Total num output samples in dev iteration 323, epoch 6: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 3649.593994140625\n",
      "Total num output samples in dev iteration 323, epoch 6: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 4752.431640625\n",
      "Total num output samples in dev iteration 323, epoch 6: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 5893.9013671875\n",
      "Total num output samples in dev iteration 323, epoch 6: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 6879.14892578125\n",
      "Total num output samples in dev iteration 323, epoch 6: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 8123.3662109375\n",
      "Total num output samples in dev iteration 323, epoch 6: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 9229.6025390625\n",
      "Total num output samples in dev iteration 323, epoch 6: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 10409.4111328125\n",
      "Total num output samples in dev iteration 323, epoch 6: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 11476.1884765625\n",
      "Total num output samples in dev iteration 323, epoch 6: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 12796.265625\n",
      "Total num output samples in dev iteration 323, epoch 6: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 14168.05859375\n",
      "Total num output samples in dev iteration 323, epoch 6: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 15374.541015625\n",
      "Total num output samples in dev iteration 323, epoch 6: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 16688.10546875\n",
      "Total num output samples in dev iteration 323, epoch 6: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 18107.328125\n",
      "Total num output samples in dev iteration 323, epoch 6: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 19362.96875\n",
      "Total num output samples in dev iteration 323, epoch 6: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 20637.80859375\n",
      "Total num output samples in dev iteration 323, epoch 6: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 22079.86328125\n",
      "Total num output samples in dev iteration 323, epoch 6: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 23288.41796875\n",
      "Total num output samples in dev iteration 323, epoch 6: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 24391.73046875\n",
      "Total num output samples in dev iteration 323, epoch 6: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 25438.2578125\n",
      "Total num output samples in dev iteration 323, epoch 6: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 26592.794921875\n",
      "Total num output samples in dev iteration 323, epoch 6: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 27972.685546875\n",
      "Total num output samples in dev iteration 323, epoch 6: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 29209.419921875\n",
      "Total num output samples in dev iteration 323, epoch 6: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 30303.896484375\n",
      "Total num output samples in dev iteration 323, epoch 6: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 31573.23046875\n",
      "Total num output samples in dev iteration 323, epoch 6: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 6: 32709.59375\n",
      "Total num output samples in dev iteration 323, epoch 6: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.0997999906539917\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "PPL: 1.0581; Speed: 15181.81 sps\n",
      "PPL: 1.0588; Speed: 15222.54 sps\n",
      "PPL: 1.0591; Speed: 15249.55 sps\n",
      "Total number of training labels: 4113728\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 1163.448974609375\n",
      "Total num output samples in dev iteration 323, epoch 7: 12736\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 2125.923828125\n",
      "Total num output samples in dev iteration 323, epoch 7: 25472\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 3218.59326171875\n",
      "Total num output samples in dev iteration 323, epoch 7: 38208\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 4235.8623046875\n",
      "Total num output samples in dev iteration 323, epoch 7: 50944\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 5242.73583984375\n",
      "Total num output samples in dev iteration 323, epoch 7: 63680\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 6091.85302734375\n",
      "Total num output samples in dev iteration 323, epoch 7: 76416\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 7187.97705078125\n",
      "Total num output samples in dev iteration 323, epoch 7: 89152\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 8095.84912109375\n",
      "Total num output samples in dev iteration 323, epoch 7: 101888\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 9097.0107421875\n",
      "Total num output samples in dev iteration 323, epoch 7: 114624\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 9965.720703125\n",
      "Total num output samples in dev iteration 323, epoch 7: 127360\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 11089.21875\n",
      "Total num output samples in dev iteration 323, epoch 7: 140096\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 12298.193359375\n",
      "Total num output samples in dev iteration 323, epoch 7: 152832\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 13320.1484375\n",
      "Total num output samples in dev iteration 323, epoch 7: 165568\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 14487.2421875\n",
      "Total num output samples in dev iteration 323, epoch 7: 178304\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 15660.5712890625\n",
      "Total num output samples in dev iteration 323, epoch 7: 191040\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 16730.98046875\n",
      "Total num output samples in dev iteration 323, epoch 7: 203776\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 17814.7578125\n",
      "Total num output samples in dev iteration 323, epoch 7: 216512\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 19112.580078125\n",
      "Total num output samples in dev iteration 323, epoch 7: 229248\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 20092.23828125\n",
      "Total num output samples in dev iteration 323, epoch 7: 241984\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 21017.556640625\n",
      "Total num output samples in dev iteration 323, epoch 7: 254720\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 21848.51171875\n",
      "Total num output samples in dev iteration 323, epoch 7: 267456\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 22884.923828125\n",
      "Total num output samples in dev iteration 323, epoch 7: 280192\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 24063.06640625\n",
      "Total num output samples in dev iteration 323, epoch 7: 292928\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 25154.41796875\n",
      "Total num output samples in dev iteration 323, epoch 7: 305664\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 26090.701171875\n",
      "Total num output samples in dev iteration 323, epoch 7: 318400\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 27208.423828125\n",
      "Total num output samples in dev iteration 323, epoch 7: 331136\n",
      "Total neg log likelihood in dev iteration 323, epoch 7: 28207.095703125\n",
      "Total num output samples in dev iteration 323, epoch 7: 343872\n",
      "Total number of validation labels: 343872\n",
      "Validation perplexity is 1.0855000019073486\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n"
     ]
    }
   ],
   "source": [
    "#!python {settings.PUNCTUATOR2TF2_DIR}/main.py {model_name} {n_layers} {lr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba09da9",
   "metadata": {},
   "source": [
    "Se ha generado el archivo Model_punctuator_h256_lr0.02.pcl en la raíz, con los parámetros del modelo. Ahora vamos a hacer la prediccion del archivo de test. Definimos antes la ruta del archivo puntuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f355889",
   "metadata": {
    "id": "wBxBrfunan_h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\predicted\\test.Model_punctuator_h256_lr0.02.pcl.raw.txt\n"
     ]
    }
   ],
   "source": [
    "output_predicted_file_path = os.path.join(settings.PREDICTED_DIR,'test.{}.raw.txt'.format(model_full_name))\n",
    "print(output_predicted_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6505abf",
   "metadata": {},
   "source": [
    "Realizamos la puntuación (tarda entorno a una hora con varias GPUs de alto rendimiento de Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19fc95fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrmJfAWwapez",
    "outputId": "9dd43de2-f8b0-4171-f3f1-043fbb3e0d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Loading model parameters...\n",
      "2022-06-20 07:00:08.301113: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "Vocabulary \"data/preprocessed/vocabulary\" size: 45087\n",
      "Vocabulary \"data/preprocessed/punctuations\" size: 7\n",
      "[TensorShape([45087, 256]), TensorShape([256, 7]), TensorShape([1, 7]), TensorShape([256, 512]), TensorShape([512, 512]), TensorShape([1, 512]), TensorShape([512]), TensorShape([256, 256]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([512, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([512, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256]), TensorShape([256, 512]), TensorShape([256, 512]), TensorShape([1, 512]), TensorShape([256, 256]), TensorShape([256, 256]), TensorShape([1, 256])]\n",
      "Building model...\n",
      "Processing line: 0/14382\n",
      "Processing line: 1439/14382\n",
      "Processing line: 2878/14382\n",
      "Processing line: 4317/14382\n",
      "Processing line: 5756/14382\n",
      "Processing line: 7195/14382\n",
      "Processing line: 8634/14382\n",
      "Processing line: 10073/14382\n",
      "Processing line: 11512/14382\n",
      "Processing line: 12951/14382\n",
      "Processing line: 14382/14382\n",
      "Process Finished\n"
     ]
    }
   ],
   "source": [
    "#!python {settings.PUNCTUATOR2TF2_DIR}/punctuator.py ./{model_full_name} {TEST_RAW_PATH} {output_predicted_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76544dc",
   "metadata": {},
   "source": [
    "El archivo anterior es necesario preprarlo para tokenizar los valores numéricos ya que el evaluador de los autores compara palabra a palabra y así nos evitamos errores con signos de puntuación entre caracteres, como por ejemplo numeros decimales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9546694",
   "metadata": {
    "id": "JJYwhrFearMT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\prepared\\test.Model_punctuator_h256_lr0.02.pcl.prepared.txt\n"
     ]
    }
   ],
   "source": [
    "TEST_PREDICT_PREP_PATH  = prepare_file(output_predicted_file_path,'test.{}.prepared.txt'.format(model_full_name),lowercase=False,allow_duplicates=False,token_punct=False)\n",
    "print(TEST_PREDICT_PREP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233d988",
   "metadata": {},
   "source": [
    "Procedemos a evaluar el modelo con el módulo **error_calculator.py** de los autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da288432",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IgyktXT4asz4",
    "outputId": "8d46e998-cb0d-4a96-93fa-0d91728ab3e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary \"data\\preprocessed\\punctuations\" size: 7\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "?QUESTIONMARK    56.000    26.000    35.500   \n",
      ".PERIOD          75.200    95.600    84.200   \n",
      ",COMMA           75.200    64.400    69.400   \n",
      ":COLON           38.300    36.200    37.200   \n",
      "!EXCLAMATIONMARK 6.500     5.200     5.700    \n",
      ";SEMICOLON       27.700    13.600    18.300   \n",
      "----------------------------------------------\n",
      "Overall          74.000    75.700    74.800   \n",
      "Err: 5.19%\n",
      "SER: 37.6%\n"
     ]
    }
   ],
   "source": [
    "!python {settings.PUNCTUATOR2TF2_DIR}/error_calculator.py {CHECK_PREP_PATH} {TEST_PREDICT_PREP_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304447ae",
   "metadata": {},
   "source": [
    "Los autores obtuvieron con los mismos parámetros de modelo y para el mismo dataset los siguientes resultados (evaluando sólo los signos ,COMMA ?QUESTIONMARK y .PERIOD):\n",
    "\n",
    "PUNCTUATION      | PRECISION | RECALL    | F-SCORE\n",
    "--- | --- | --- | ---\n",
    ",COMMA           | 64.4 | 45.2 | 53.1\n",
    "?QUESTIONMARK    | 67.5 | 58.7 | 62.8\n",
    ".PERIOD          | 72.3 | 71.5 | 71.9\n",
    "_Overall_        | _68.9_ | _58.1_ | _63.1_\n",
    "\n",
    "Observamos que obtenemos unas métricas superiores a las que los autores presentan (precision_overall = 74.0, recall_overall = 75,7 y F1_overall = 74.8). Esto se debe principalmente a las modificaciones realizadas en el módulo **punctuator**:\n",
    " * Ahora pone la primera letra en mayúscula.\n",
    " * Si el último token no es un signo de final de oración se añade un .PERIOD\n",
    " * La palabra tras un signo de final de oración ahora se pone en mayúscula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6d1da",
   "metadata": {},
   "source": [
    "Ahora vamos a usar la función de evaluación que definimos en la parte 1 basada en la función *VerifyPunctuation*. Para ello importamos versiones de estas funciones que en lugar de hacer la predicción in situ como en el apartado 1, toman como entradas archivos ya puntuados por un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "316c7a13",
   "metadata": {
    "id": "PACyVs4SaZmM"
   },
   "outputs": [],
   "source": [
    "from evaluator import evaluate_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69bfc584",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGHU414kawfb",
    "outputId": "cdcd231b-7e26-479a-931b-46fc150089e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "MÉTRICAS\n",
      "=============================================\n",
      "precision global:  0.7891876531808333\n",
      "recall global:  0.6978234241025157\n",
      "F1 global:  0.7406987598346447\n",
      "=============================================\n",
      "precision media:  0.8370715261265561\n",
      "recall medio:  0.7736186422895313\n",
      "F1 medio:  0.8040952260583556\n",
      "=============================================\n",
      "rendimiento:  0.3415380336531776\n",
      "=============================================\n",
      "número de instancias en el corpus:  14382\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_global': 0.7891876531808333,\n",
       " 'recall_global': 0.6978234241025157,\n",
       " 'F1_global': 0.7406987598346447,\n",
       " 'precision_mean': 0.8370715261265561,\n",
       " 'recall_mean': 0.7736186422895313,\n",
       " 'F1_mean': 0.8040952260583556,\n",
       " 'score': 0.3415380336531776}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_punctuated(CHECK_PREP_PATH,TEST_RAW_PATH,TEST_PREDICT_PREP_PATH,prepared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23b1f7",
   "metadata": {},
   "source": [
    "Las métricas obtenidas por nuestro evaluador son bastante parecidas a las obtenidas por los autores, teniendo en cuenta que el cómputo se hace distinto y ellos las calculan signo a signo. En cualquier caso, el resultado es bastante bueno y mejor que cualquiera de los modelos de la parte 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dab69e",
   "metadata": {},
   "source": [
    "Para finalizar, podemos explorar las distintas predicciones hechas por este modelo usando la versión adaptada a entrada de archivo de *evaluate_from_corpus* del apartado 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2ff9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate_example_from_corpus_punctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ef062ed",
   "metadata": {
    "id": "N-rTszQTOSdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST LINE: \n",
      "  and the world production today everybody combined making laptops is five million a month so i'm standing here telling you that sometime next year we're going to make 20 percent of the world production\n",
      "MODEL PUNCTUATED LINE: \n",
      "  And the world production today, everybody combined, making laptops. Is five million a month. So i'm standing here telling you that sometime next year, we're going to make <NUM> percent of the world production.\n",
      "VALIDATION LINE: \n",
      "  And the world production today, everybody combined, making laptops, is five million a month. So I'm standing here telling you that sometime next year, we're going to make <NUM> percent of the world production. \n",
      "\n",
      "Modificaciones necesarias:  {('I', 5), ('I', 34), ('S', 28), ('I', 24), ('I', 14), ('S', 15), ('I', 7), ('S', 14), ('I', 9), ('S', 0)}\n",
      "Modificaciones hechas por el modelo:  {('S', 9), ('I', 5), ('I', 34), ('S', 28), ('I', 24), ('I', 14), ('I', 7), ('S', 14), ('I', 9), ('S', 0)}\n",
      "Diferencias entre modelo y validación:  {('S', 12), ('S', 19), ('S', 11)} \n",
      "\n",
      "n_hechas (Núm. de modificaciones hechas por el modelo):  10\n",
      "n_correctas (Núm. de modificaciones correctas: \n",
      " interseccion(hechas,necesarias) - error de sustitucion de signo):  9\n",
      "n_necesarias (Núm. de modificaciones necesarias):  10 \n",
      "\n",
      "precision (n_correctas/n_hechas):  0.9\n",
      "recall (n_correctas/n_necesarias):  0.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_line = 88\n",
    "evaluate_example_from_corpus_punctuated(CHECK_PREP_PATH,TEST_RAW_PATH,TEST_PREDICT_PREP_PATH,corpus_line,prepared=True, readeable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae151a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf8521805df2f681626d7ffc655f9a444f7479c982c19b2313015333928fe69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
